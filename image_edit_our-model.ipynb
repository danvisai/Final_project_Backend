{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e0844f-ceb9-4301-8217-1d65aabdf69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-02 07:37:28--  https://huggingface.co/dome272/wuerstchen/resolve/main/model_v2_stage_b.pt\n",
      "Resolving huggingface.co (huggingface.co)... 52.85.242.84, 52.85.242.16, 52.85.242.35, ...\n",
      "Connecting to huggingface.co (huggingface.co)|52.85.242.84|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/aba31f166a8dfb672fc81b63336b12e4667a10ddfb783a822b8fe20b356a899c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_v2_stage_b.pt%3B+filename%3D%22model_v2_stage_b.pt%22%3B&Expires=1701761849&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTc2MTg0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2FiYTMxZjE2NmE4ZGZiNjcyZmM4MWI2MzMzNmIxMmU0NjY3YTEwZGRmYjc4M2E4MjJiOGZlMjBiMzU2YTg5OWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=SBQnZIV1zYyYPQzFD73H6KC9dF-sTES4vVAYAjFuBppCCHTTHHEvHYWYWUEHL20pFnbSCIsjuZMNlcbZHO5K2HI3Tsga2mndOMH4E5uI8%7EPNlAQFFhZ4zfdtgzwo-zFobTjVGuc1KjeV5lL7IZ1%7EN3dNjxYzsqilWBptsbo1B9eJeqBjuWq5-2xGOt-OMrj0nW6jJ%7EiDIzYLsnpbqzxem8KibgSny2cW37R3%7Esxv5Y2IQmEAZ7LrvSWzlJrw23qRNBKOqIEPn8hgHOHYL0SodjQiYtyOcRDh%7EGE4fi8FF-eWjG1D5yTvwAa9XJi-GhtOfgpNRgIkDjZhDQtChaCVpQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-12-02 07:37:29--  https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/aba31f166a8dfb672fc81b63336b12e4667a10ddfb783a822b8fe20b356a899c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_v2_stage_b.pt%3B+filename%3D%22model_v2_stage_b.pt%22%3B&Expires=1701761849&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTc2MTg0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2FiYTMxZjE2NmE4ZGZiNjcyZmM4MWI2MzMzNmIxMmU0NjY3YTEwZGRmYjc4M2E4MjJiOGZlMjBiMzU2YTg5OWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=SBQnZIV1zYyYPQzFD73H6KC9dF-sTES4vVAYAjFuBppCCHTTHHEvHYWYWUEHL20pFnbSCIsjuZMNlcbZHO5K2HI3Tsga2mndOMH4E5uI8%7EPNlAQFFhZ4zfdtgzwo-zFobTjVGuc1KjeV5lL7IZ1%7EN3dNjxYzsqilWBptsbo1B9eJeqBjuWq5-2xGOt-OMrj0nW6jJ%7EiDIzYLsnpbqzxem8KibgSny2cW37R3%7Esxv5Y2IQmEAZ7LrvSWzlJrw23qRNBKOqIEPn8hgHOHYL0SodjQiYtyOcRDh%7EGE4fi8FF-eWjG1D5yTvwAa9XJi-GhtOfgpNRgIkDjZhDQtChaCVpQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.214.7, 108.157.214.46, 108.157.214.31, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.214.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4303483515 (4.0G) [application/zip]\n",
      "Saving to: ‘models/model_v2_stage_b.pt’\n",
      "\n",
      "model_v2_stage_b.pt 100%[===================>]   4.01G   111MB/s    in 39s     \n",
      "\n",
      "2023-12-02 07:38:08 (105 MB/s) - ‘models/model_v2_stage_b.pt’ saved [4303483515/4303483515]\n",
      "\n",
      "--2023-12-02 07:38:08--  https://huggingface.co/dome272/wuerstchen/resolve/main/model_v2_stage_c_finetune_interpolation.pt\n",
      "Resolving huggingface.co (huggingface.co)... 52.85.242.8, 52.85.242.35, 52.85.242.16, ...\n",
      "Connecting to huggingface.co (huggingface.co)|52.85.242.8|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/d7178185bb6bf26a975e3afabe9baeacbc6424e97f636b97326d53043fd86ea5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_v2_stage_c_finetune_interpolation.pt%3B+filename%3D%22model_v2_stage_c_finetune_interpolation.pt%22%3B&Expires=1701761888&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTc2MTg4OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2Q3MTc4MTg1YmI2YmYyNmE5NzVlM2FmYWJlOWJhZWFjYmM2NDI0ZTk3ZjYzNmI5NzMyNmQ1MzA0M2ZkODZlYTU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=YpApKcjcAbbDoIElbuSGHs8JrLGbHXJyUVRD8-bVZEMVzOF0en2cMH--a5iWOEyk93yi0G4dHz4yE6KXCxcq3ea03jtmSCTVCxFH7JBoW0EnsAR7-A-idUtJUlfuvwurMdYvLosdj3o1K38asN6PPvVMOqPCjXtwepjXAiDAouBA8eG3SKuTXEnrhQ%7EMHGURuP20SPtwR84J3xAsAo7RD1CxqvMjOLyKSXpUBEsAaAOO9OJAppg%7EjoVSmqnOFSv85cvptMgAKJPldIuVxmBeQzX1FFOLhaO6w2puHCrFYCtVR7htyxrCL1TzufNeMn2ZQYQwuQIqCUIaglCPPolwKA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-12-02 07:38:08--  https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/d7178185bb6bf26a975e3afabe9baeacbc6424e97f636b97326d53043fd86ea5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_v2_stage_c_finetune_interpolation.pt%3B+filename%3D%22model_v2_stage_c_finetune_interpolation.pt%22%3B&Expires=1701761888&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTc2MTg4OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2Q3MTc4MTg1YmI2YmYyNmE5NzVlM2FmYWJlOWJhZWFjYmM2NDI0ZTk3ZjYzNmI5NzMyNmQ1MzA0M2ZkODZlYTU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=YpApKcjcAbbDoIElbuSGHs8JrLGbHXJyUVRD8-bVZEMVzOF0en2cMH--a5iWOEyk93yi0G4dHz4yE6KXCxcq3ea03jtmSCTVCxFH7JBoW0EnsAR7-A-idUtJUlfuvwurMdYvLosdj3o1K38asN6PPvVMOqPCjXtwepjXAiDAouBA8eG3SKuTXEnrhQ%7EMHGURuP20SPtwR84J3xAsAo7RD1CxqvMjOLyKSXpUBEsAaAOO9OJAppg%7EjoVSmqnOFSv85cvptMgAKJPldIuVxmBeQzX1FFOLhaO6w2puHCrFYCtVR7htyxrCL1TzufNeMn2ZQYQwuQIqCUIaglCPPolwKA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.214.31, 108.157.214.7, 108.157.214.46, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.214.31|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3974759010 (3.7G) [application/zip]\n",
      "Saving to: ‘models/model_v2_stage_c_finetune_interpolation.pt’\n",
      "\n",
      "model_v2_stage_c_fi 100%[===================>]   3.70G  87.6MB/s    in 37s     \n",
      "\n",
      "2023-12-02 07:38:45 (104 MB/s) - ‘models/model_v2_stage_c_finetune_interpolation.pt’ saved [3974759010/3974759010]\n",
      "\n",
      "--2023-12-02 07:38:45--  https://huggingface.co/dome272/wuerstchen/resolve/main/vqgan_f4_v1_500k.pt\n",
      "Resolving huggingface.co (huggingface.co)... 52.85.242.8, 52.85.242.35, 52.85.242.84, ...\n",
      "Connecting to huggingface.co (huggingface.co)|52.85.242.8|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/b4a81dd8733268a68d08844e33b917d4dbe27d5c5382dcbde522df3af16f343e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vqgan_f4_v1_500k.pt%3B+filename%3D%22vqgan_f4_v1_500k.pt%22%3B&Expires=1701761925&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTc2MTkyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2I0YTgxZGQ4NzMzMjY4YTY4ZDA4ODQ0ZTMzYjkxN2Q0ZGJlMjdkNWM1MzgyZGNiZGU1MjJkZjNhZjE2ZjM0M2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=UYUk2bqrlYrYbyJXAja9OFAkaqhk7vnR423JIgSA7xvDbFqVx8DJNA6wgZIvrPfSbjUXFkmHBftkaq29xA8VE1aklcMjAPFmEi0-GHyvYGNB%7EGn24KzASMw1FoLdbg0eADPND8dgHM9yZfzktR8kJuj0nGHo5O0QInVRHAPr3c9XwUJ7dc3yH3Otm%7ETQINDladtrZ9hMCo6uDcxALfD%7Em9t4BOSQ4mXRW2uWnMfZXnHwNDXjGJELhO1%7EN0j8Lj2H1xwHAB2pNdYalryShg4Yu-s6dI5QTISPu64xjZ3R7wrbNyxqyRmFi2hgFn2tAIzCc%7EEcwhgPopE7lWMqbtie7A__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-12-02 07:38:45--  https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/b4a81dd8733268a68d08844e33b917d4dbe27d5c5382dcbde522df3af16f343e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vqgan_f4_v1_500k.pt%3B+filename%3D%22vqgan_f4_v1_500k.pt%22%3B&Expires=1701761925&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTc2MTkyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2I0YTgxZGQ4NzMzMjY4YTY4ZDA4ODQ0ZTMzYjkxN2Q0ZGJlMjdkNWM1MzgyZGNiZGU1MjJkZjNhZjE2ZjM0M2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=UYUk2bqrlYrYbyJXAja9OFAkaqhk7vnR423JIgSA7xvDbFqVx8DJNA6wgZIvrPfSbjUXFkmHBftkaq29xA8VE1aklcMjAPFmEi0-GHyvYGNB%7EGn24KzASMw1FoLdbg0eADPND8dgHM9yZfzktR8kJuj0nGHo5O0QInVRHAPr3c9XwUJ7dc3yH3Otm%7ETQINDladtrZ9hMCo6uDcxALfD%7Em9t4BOSQ4mXRW2uWnMfZXnHwNDXjGJELhO1%7EN0j8Lj2H1xwHAB2pNdYalryShg4Yu-s6dI5QTISPu64xjZ3R7wrbNyxqyRmFi2hgFn2tAIzCc%7EEcwhgPopE7lWMqbtie7A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.214.7, 108.157.214.46, 108.157.214.31, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.214.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 296367311 (283M) [binary/octet-stream]\n",
      "Saving to: ‘models/vqgan_f4_v1_500k.pt’\n",
      "\n",
      "vqgan_f4_v1_500k.pt 100%[===================>] 282.64M  97.8MB/s    in 2.9s    \n",
      "\n",
      "2023-12-02 07:38:48 (97.8 MB/s) - ‘models/vqgan_f4_v1_500k.pt’ saved [296367311/296367311]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "!wget -P models https://huggingface.co/dome272/wuerstchen/resolve/main/model_v2_stage_b.pt\n",
    "!wget -P models https://huggingface.co/dome272/wuerstchen/resolve/main/model_v2_stage_c_finetune_interpolation.pt\n",
    "!wget -P models https://huggingface.co/dome272/wuerstchen/resolve/main/vqgan_f4_v1_500k.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f89f7ec-6252-4958-a5ac-48007ffcabe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/pabloppp/pytorch-tools\n",
      "  Cloning https://github.com/pabloppp/pytorch-tools to /tmp/pip-req-build-_ntwp02o\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/pabloppp/pytorch-tools /tmp/pip-req-build-_ntwp02o\n",
      "  Resolved https://github.com/pabloppp/pytorch-tools to commit 49d0c9234f5f2fed65c5bae3313767af10af2b3d\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from torchtools==0.3.8) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from torchtools==0.3.8) (0.16.0+cu118)\n",
      "Collecting ninja>=1.0 (from torchtools==0.3.8)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Downloading fsspec-2023.12.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.8) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.8) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.8) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.8) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->torchtools==0.3.8) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->torchtools==0.3.8) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->torchtools==0.3.8) (1.3.0)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m188.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m239.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m186.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.12.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: torchtools\n",
      "  Building wheel for torchtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchtools: filename=torchtools-0.3.8-py3-none-any.whl size=454024 sha256=9c79531e60fd15f3e9b2be036843bba6969afa2cea5d900b9ab6ca6acd911d7c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-h6o5qypx/wheels/8a/54/d6/da73a3a6b36c3f9309983d5e7665a21e9154ac5a9bff318fc9\n",
      "Successfully built torchtools\n",
      "Installing collected packages: ninja, tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers, torchtools\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2023.12.1 huggingface-hub-0.19.4 ninja-1.11.1.1 regex-2023.10.3 safetensors-0.4.1 tokenizers-0.15.0 torchtools-0.3.8 tqdm-4.66.1 transformers-4.35.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers git+https://github.com/pabloppp/pytorch-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc6c556-aae2-453c-ae23-3e435ed8f593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.46.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.2/156.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.46.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m219.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.46.0 kiwisolver-1.4.5 matplotlib-3.8.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448ecc6a-a210-4ee1-8379-6afafee2049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "#                        Code taken from Pablos implementation:                             #\n",
    "#    https://github.com/pabloppp/pytorch-tools/blob/master/torchtools/utils/diffusion.py    #\n",
    "#############################################################################################\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# Samplers --------------------------------------------------------------------\n",
    "class SimpleSampler():\n",
    "    def __init__(self, diffuzz):\n",
    "        self.current_step = -1\n",
    "        self.diffuzz = diffuzz\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.current_step += 1\n",
    "        return self.step(*args, **kwargs)\n",
    "\n",
    "    def init_x(self, shape):\n",
    "        return torch.randn(*shape, device=self.diffuzz.device)\n",
    "\n",
    "    def step(self, x, t, t_prev, noise):\n",
    "        raise NotImplementedError(\"You should override the 'apply' function.\")\n",
    "\n",
    "\n",
    "class DDPMSampler(SimpleSampler):\n",
    "    def step(self, x, t, t_prev, noise):\n",
    "        alpha_cumprod = self.diffuzz._alpha_cumprod(t).view(t.size(0), *[1 for _ in x.shape[1:]])\n",
    "        alpha_cumprod_prev = self.diffuzz._alpha_cumprod(t_prev).view(t_prev.size(0), *[1 for _ in x.shape[1:]])\n",
    "        alpha = (alpha_cumprod / alpha_cumprod_prev)\n",
    "\n",
    "        mu = (1.0 / alpha).sqrt() * (x - (1 - alpha) * noise / (1 - alpha_cumprod).sqrt())\n",
    "        std = ((1 - alpha) * (1. - alpha_cumprod_prev) / (1. - alpha_cumprod)).sqrt() * torch.randn_like(mu)\n",
    "        return mu + std * (t_prev != 0).float().view(t_prev.size(0), *[1 for _ in x.shape[1:]])\n",
    "\n",
    "\n",
    "class DDIMSampler(SimpleSampler):\n",
    "    def step(self, x, t, t_prev, noise):\n",
    "        alpha_cumprod = self.diffuzz._alpha_cumprod(t).view(t.size(0), *[1 for _ in x.shape[1:]])\n",
    "        alpha_cumprod_prev = self.diffuzz._alpha_cumprod(t_prev).view(t_prev.size(0), *[1 for _ in x.shape[1:]])\n",
    "\n",
    "        x0 = (x - (1 - alpha_cumprod).sqrt() * noise) / (alpha_cumprod).sqrt()\n",
    "        dp_xt = (1 - alpha_cumprod_prev).sqrt()\n",
    "        return (alpha_cumprod_prev).sqrt() * x0 + dp_xt * noise\n",
    "\n",
    "\n",
    "sampler_dict = {\n",
    "    'ddpm': DDPMSampler,\n",
    "    'ddim': DDIMSampler,\n",
    "}\n",
    "\n",
    "\n",
    "# Custom simplified foward/backward diffusion (cosine schedule)\n",
    "class Diffuzz():\n",
    "    def __init__(self, s=0.008, device=\"cpu\", cache_steps=None, scaler=1):\n",
    "        self.device = device\n",
    "        self.s = torch.tensor([s]).to(device)\n",
    "        self._init_alpha_cumprod = torch.cos(self.s / (1 + self.s) * torch.pi * 0.5) ** 2\n",
    "        self.scaler = scaler\n",
    "        self.cached_steps = None\n",
    "        if cache_steps is not None:\n",
    "            self.cached_steps = self._alpha_cumprod(torch.linspace(0, 1, cache_steps, device=device))\n",
    "\n",
    "    def _alpha_cumprod(self, t):\n",
    "        if self.cached_steps is None:\n",
    "            if self.scaler > 1:\n",
    "                t = 1 - (1 - t) ** self.scaler\n",
    "            elif self.scaler < 1:\n",
    "                t = t ** self.scaler\n",
    "            alpha_cumprod = torch.cos((t + self.s) / (1 + self.s) * torch.pi * 0.5) ** 2 / self._init_alpha_cumprod\n",
    "            return alpha_cumprod.clamp(0.0001, 0.9999)\n",
    "        else:\n",
    "            return self.cached_steps[t.mul(len(self.cached_steps) - 1).long()]\n",
    "\n",
    "    def diffuse(self, x, t, noise=None):  # t -> [0, 1]\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x)\n",
    "        alpha_cumprod = self._alpha_cumprod(t).view(t.size(0), *[1 for _ in x.shape[1:]])\n",
    "        return alpha_cumprod.sqrt() * x + (1 - alpha_cumprod).sqrt() * noise, noise\n",
    "\n",
    "    def undiffuse(self, x, t, t_prev, noise, sampler=None):\n",
    "        if sampler is None:\n",
    "            sampler = DDPMSampler(self)\n",
    "        return sampler(x, t, t_prev, noise)\n",
    "\n",
    "    def sample(self, model, model_inputs, shape, mask=None, t_start=1.0, t_end=0.0, timesteps=20, x_init=None, cfg=3.0,\n",
    "               unconditional_inputs=None, sampler='ddpm', half=False, targetX=None, targetAlphas=None):\n",
    "        r_range = torch.linspace(t_start, t_end, timesteps + 1)[:, None].expand(-1, shape[\n",
    "            0] if x_init is None else x_init.size(0)).to(self.device)\n",
    "        if isinstance(sampler, str):\n",
    "            if sampler in sampler_dict:\n",
    "                sampler = sampler_dict[sampler](self)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"If sampler is a string it must be one of the supported samplers: {list(sampler_dict.keys())}\")\n",
    "        elif issubclass(sampler, SimpleSampler):\n",
    "            sampler = sampler(self)\n",
    "        else:\n",
    "            raise ValueError(\"Sampler should be either a string or a SimpleSampler object.\")\n",
    "        preds = []\n",
    "        attentions=[]\n",
    "        attention = None\n",
    "        x = sampler.init_x(shape) if x_init is None or mask is not None else x_init.clone()\n",
    "        print(x.size())\n",
    "        if half:\n",
    "            r_range = r_range.half()\n",
    "            x = x.half()\n",
    "        if cfg is not None:\n",
    "            if unconditional_inputs is None:\n",
    "                unconditional_inputs = {k: torch.zeros_like(v) for k, v in model_inputs.items()}\n",
    "            model_inputs = {k: torch.cat([v, v_u]) for (k, v), (k_u, v_u) in\n",
    "                            zip(model_inputs.items(), unconditional_inputs.items())}\n",
    "\n",
    "        for i in range(0, timesteps):\n",
    "            if mask is not None and x_init is not None:\n",
    "                x_renoised, _ = self.diffuse(x_init, r_range[i])\n",
    "                x = x * mask + x_renoised * (1 - mask)\n",
    "        \n",
    "            modified_model_inputs = model_inputs.copy()\n",
    "            if 'originalAttentions' in modified_model_inputs:\n",
    "                modified_model_inputs['originalAttentions'] = modified_model_inputs['originalAttentions'][i]\n",
    "        \n",
    "            if cfg is not None:\n",
    "                output, attention = model(torch.cat([x] * 2), torch.cat([r_range[i]] * 2), **modified_model_inputs)\n",
    "                pred_noise, pred_noise_unconditional = output.chunk(2)\n",
    "                pred_noise = torch.lerp(pred_noise_unconditional, pred_noise, cfg)\n",
    "            else:\n",
    "                if isinstance(model, Prior):\n",
    "                    pred_noise, attention = model(x, r_range[i], **modified_model_inputs)\n",
    "                else:\n",
    "                    pred_noise = model(x, r_range[i], **modified_model_inputs)\n",
    "        \n",
    "            x = self.undiffuse(x, r_range[i], r_range[i + 1], pred_noise, sampler=sampler)\n",
    "\n",
    "            if targetX is not None:\n",
    "                # Move x a fraction of the way towards targetX\n",
    "                x = x + targetAlphas[i] * (targetX - x)\n",
    "        \n",
    "            preds.append(x)\n",
    "            attentions.append(attention)\n",
    "        return preds, attentions\n",
    "\n",
    "    def p2_weight(self, t, k=1.0, gamma=1.0):\n",
    "        alpha_cumprod = self._alpha_cumprod(t)\n",
    "        return (k + alpha_cumprod / (1 - alpha_cumprod)) ** -gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45063596-5b05-49ad-ac4c-f30d0c749d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class GlobalResponseNorm(nn.Module):  # from https://github.com/facebookresearch/ConvNeXt-V2/blob/3608f67cc1dae164790c5d0aead7bf2d73d9719b/models/utils.py#L105\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        return self.gamma * (x * Nx) + self.beta + x\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, c, c_cond, nhead, self_attn=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.norm = LayerNorm2d(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.attention = Attention2D(c, nhead, dropout)\n",
    "        self.kv_mapper = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(c_cond, c)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, kv):\n",
    "        kv = self.kv_mapper(kv)\n",
    "        x = x + self.attention(self.norm(x), kv, self_attn=self.self_attn)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, c, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm2d(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.channelwise = nn.Sequential(\n",
    "            nn.Linear(c, c * 4),\n",
    "            nn.GELU(),\n",
    "            GlobalResponseNorm(c * 4),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(c * 4, c)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.channelwise(self.norm(x).permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    def __init__(self, c, c_timestep):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Linear(c_timestep, c * 2)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        a, b = self.mapper(t)[:, :, None, None].chunk(2, dim=1)\n",
    "        return x * (1 + a) + b\n",
    "\n",
    "\n",
    "class Attention2D(nn.Module):\n",
    "    def __init__(self, c, nhead, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(c, nhead, dropout=dropout, bias=True, batch_first=True)\n",
    "\n",
    "    def forward(self, x, kv, self_attn=False):\n",
    "        orig_shape = x.shape\n",
    "        x = x.view(x.size(0), x.size(1), -1).permute(0, 2, 1)  # Bx4xHxW -> Bx(HxW)x4\n",
    "        if self_attn:\n",
    "            kv = torch.cat([x, kv], dim=1)\n",
    "        x = self.attn(x, kv, kv, need_weights=False)[0]\n",
    "        x = x.permute(0, 2, 1).view(*orig_shape)\n",
    "        return x\n",
    "\n",
    "class ResBlockStageB(nn.Module):\n",
    "    def __init__(self, c, c_skip=None, kernel_size=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(c, c, kernel_size=kernel_size, padding=kernel_size//2, groups=c)\n",
    "        self.norm = LayerNorm2d(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.channelwise = nn.Sequential(\n",
    "            nn.Linear(c+c_skip, c*4),\n",
    "            nn.GELU(),\n",
    "            GlobalResponseNorm(c*4),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(c*4, c)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_skip=None):\n",
    "        x_res = x\n",
    "        x = self.norm(self.depthwise(x))\n",
    "        if x_skip is not None:\n",
    "            x = torch.cat([x, x_skip], dim=1)\n",
    "        x = self.channelwise(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        return x + x_res\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c, c_skip=0, kernel_size=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(c + c_skip, c, kernel_size=kernel_size, padding=kernel_size // 2, groups=c)\n",
    "        self.norm = LayerNorm2d(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.channelwise = nn.Sequential(\n",
    "            nn.Linear(c, c * 4),\n",
    "            nn.GELU(),\n",
    "            GlobalResponseNorm(c * 4),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(c * 4, c)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_skip=None):\n",
    "        x_res = x\n",
    "        if x_skip is not None:\n",
    "            x = torch.cat([x, x_skip], dim=1)\n",
    "        x = self.norm(self.depthwise(x)).permute(0, 2, 3, 1)\n",
    "        x = self.channelwise(x).permute(0, 3, 1, 2)\n",
    "        return x + x_res\n",
    "\n",
    "\n",
    "class DiffNeXt(nn.Module):\n",
    "    def __init__(self, c_in=4, c_out=4, c_r=64, patch_size=2, c_cond=1024, c_hidden=[320, 640, 1280, 1280],\n",
    "                 nhead=[-1, 10, 20, 20], blocks=[4, 4, 14, 4], level_config=['CT', 'CTA', 'CTA', 'CTA'],\n",
    "                 inject_effnet=[False, True, True, True], effnet_embd=16, clip_embd=1024, kernel_size=3, dropout=0.1,\n",
    "                 self_attn=True):\n",
    "        super().__init__()\n",
    "        self.c_r = c_r\n",
    "        self.c_cond = c_cond\n",
    "        if not isinstance(dropout, list):\n",
    "            dropout = [dropout] * len(c_hidden)\n",
    "\n",
    "        # CONDITIONING\n",
    "        self.clip_mapper = nn.Linear(clip_embd, c_cond)\n",
    "        self.effnet_mappers = nn.ModuleList([\n",
    "            nn.Conv2d(effnet_embd, c_cond, kernel_size=1) if inject else None for inject in\n",
    "            inject_effnet + list(reversed(inject_effnet))\n",
    "        ])\n",
    "        self.seq_norm = nn.LayerNorm(c_cond, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.PixelUnshuffle(patch_size),\n",
    "            nn.Conv2d(c_in * (patch_size ** 2), c_hidden[0], kernel_size=1),\n",
    "            LayerNorm2d(c_hidden[0], elementwise_affine=False, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        def get_block(block_type, c_hidden, nhead, c_skip=0, dropout=0):\n",
    "            if block_type == 'C':\n",
    "                return ResBlockStageB(c_hidden, c_skip, kernel_size=kernel_size, dropout=dropout)\n",
    "            elif block_type == 'A':\n",
    "                return AttnBlock(c_hidden, c_cond, nhead, self_attn=self_attn, dropout=dropout)\n",
    "            elif block_type == 'F':\n",
    "                return FeedForwardBlock(c_hidden, dropout=dropout)\n",
    "            elif block_type == 'T':\n",
    "                return TimestepBlock(c_hidden, c_r)\n",
    "            else:\n",
    "                raise Exception(f'Block type {block_type} not supported')\n",
    "\n",
    "        # BLOCKS\n",
    "        # -- down blocks\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        for i in range(len(c_hidden)):\n",
    "            down_block = nn.ModuleList()\n",
    "            if i > 0:\n",
    "                down_block.append(nn.Sequential(\n",
    "                    LayerNorm2d(c_hidden[i - 1], elementwise_affine=False, eps=1e-6),\n",
    "                    nn.Conv2d(c_hidden[i - 1], c_hidden[i], kernel_size=2, stride=2),\n",
    "                ))\n",
    "            for _ in range(blocks[i]):\n",
    "                for block_type in level_config[i]:\n",
    "                    c_skip = c_cond if inject_effnet[i] else 0\n",
    "                    down_block.append(get_block(block_type, c_hidden[i], nhead[i], c_skip=c_skip, dropout=dropout[i]))\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # -- up blocks\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i in reversed(range(len(c_hidden))):\n",
    "            up_block = nn.ModuleList()\n",
    "            for j in range(blocks[i]):\n",
    "                for k, block_type in enumerate(level_config[i]):\n",
    "                    c_skip = c_hidden[i] if i < len(c_hidden) - 1 and j == k == 0 else 0\n",
    "                    c_skip += c_cond if inject_effnet[i] else 0\n",
    "                    up_block.append(get_block(block_type, c_hidden[i], nhead[i], c_skip=c_skip, dropout=dropout[i]))\n",
    "            if i > 0:\n",
    "                up_block.append(nn.Sequential(\n",
    "                    LayerNorm2d(c_hidden[i], elementwise_affine=False, eps=1e-6),\n",
    "                    nn.ConvTranspose2d(c_hidden[i], c_hidden[i - 1], kernel_size=2, stride=2),\n",
    "                ))\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # OUTPUT\n",
    "        self.clf = nn.Sequential(\n",
    "            LayerNorm2d(c_hidden[0], elementwise_affine=False, eps=1e-6),\n",
    "            nn.Conv2d(c_hidden[0], 2 * c_out * (patch_size ** 2), kernel_size=1),\n",
    "            nn.PixelShuffle(patch_size),\n",
    "        )\n",
    "\n",
    "        # --- WEIGHT INIT ---\n",
    "        self.apply(self._init_weights)  # General init\n",
    "        for mapper in self.effnet_mappers:\n",
    "            if mapper is not None:\n",
    "                nn.init.normal_(mapper.weight, std=0.02)  # conditionings\n",
    "        nn.init.normal_(self.clip_mapper.weight, std=0.02)  # conditionings\n",
    "        nn.init.xavier_uniform_(self.embedding[1].weight, 0.02)  # inputs\n",
    "        nn.init.constant_(self.clf[1].weight, 0)  # outputs\n",
    "\n",
    "        # blocks\n",
    "        for level_block in self.down_blocks + self.up_blocks:\n",
    "            for block in level_block:\n",
    "                if isinstance(block, ResBlockStageB) or isinstance(block, FeedForwardBlock):\n",
    "                    block.channelwise[-1].weight.data *= np.sqrt(1 / sum(blocks))\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    nn.init.constant_(block.mapper.weight, 0)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def gen_r_embedding(self, r, max_positions=10000):\n",
    "        r = r * max_positions\n",
    "        half_dim = self.c_r // 2\n",
    "        emb = math.log(max_positions) / (half_dim - 1)\n",
    "        emb = torch.arange(half_dim, device=r.device).float().mul(-emb).exp()\n",
    "        emb = r[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "        if self.c_r % 2 == 1:  # zero pad\n",
    "            emb = nn.functional.pad(emb, (0, 1), mode='constant')\n",
    "        return emb\n",
    "\n",
    "    def gen_c_embeddings(self, clip):\n",
    "        clip = self.clip_mapper(clip)\n",
    "        clip = self.seq_norm(clip)\n",
    "        return clip\n",
    "\n",
    "    def _down_encode(self, x, r_embed, effnet, clip):\n",
    "        level_outputs = []\n",
    "        for i, down_block in enumerate(self.down_blocks):\n",
    "            effnet_c = None\n",
    "            for block in down_block:\n",
    "                if isinstance(block, ResBlockStageB):\n",
    "                    if effnet_c is None and self.effnet_mappers[i] is not None:\n",
    "                        effnet_c = self.effnet_mappers[i](nn.functional.interpolate(\n",
    "                            effnet.float(), size=x.shape[-2:], mode='bicubic', antialias=True, align_corners=True\n",
    "                        ))\n",
    "                    skip = effnet_c if self.effnet_mappers[i] is not None else None\n",
    "                    x = block(x, skip)\n",
    "                elif isinstance(block, AttnBlock):\n",
    "                    x = block(x, clip)\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    x = block(x, r_embed)\n",
    "                else:\n",
    "                    x = block(x)\n",
    "            level_outputs.insert(0, x)\n",
    "        return level_outputs\n",
    "\n",
    "    def _up_decode(self, level_outputs, r_embed, effnet, clip):\n",
    "        x = level_outputs[0]\n",
    "        for i, up_block in enumerate(self.up_blocks):\n",
    "            effnet_c = None\n",
    "            for j, block in enumerate(up_block):\n",
    "                if isinstance(block, ResBlockStageB):\n",
    "                    if effnet_c is None and self.effnet_mappers[len(self.down_blocks) + i] is not None:\n",
    "                        effnet_c = self.effnet_mappers[len(self.down_blocks) + i](nn.functional.interpolate(\n",
    "                            effnet.float(), size=x.shape[-2:], mode='bicubic', antialias=True, align_corners=True\n",
    "                        ))\n",
    "                    skip = level_outputs[i] if j == 0 and i > 0 else None\n",
    "                    if effnet_c is not None:\n",
    "                        if skip is not None:\n",
    "                            skip = torch.cat([skip, effnet_c], dim=1)\n",
    "                        else:\n",
    "                            skip = effnet_c\n",
    "                    x = block(x, skip)\n",
    "                elif isinstance(block, AttnBlock):\n",
    "                    x = block(x, clip)\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    x = block(x, r_embed)\n",
    "                else:\n",
    "                    x = block(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, r, effnet, clip, x_cat=None, eps=1e-3, return_noise=True):\n",
    "        if x_cat is not None:\n",
    "            x = torch.cat([x, x_cat], dim=1)\n",
    "        # Process the conditioning embeddings\n",
    "        r_embed = self.gen_r_embedding(r)\n",
    "        clip = self.gen_c_embeddings(clip)\n",
    "\n",
    "        # Model Blocks\n",
    "        x_in = x\n",
    "        x = self.embedding(x)\n",
    "        level_outputs = self._down_encode(x, r_embed, effnet, clip)\n",
    "        x = self._up_decode(level_outputs, r_embed, effnet, clip)\n",
    "        a, b = self.clf(x).chunk(2, dim=1)\n",
    "        b = b.sigmoid() * (1 - eps * 2) + eps\n",
    "        if return_noise:\n",
    "            return (x_in - a) / b\n",
    "        else:\n",
    "            return a, b\n",
    "\n",
    "    def update_weights_ema(self, src_model, beta=0.999):\n",
    "        for self_params, src_params in zip(self.parameters(), src_model.parameters()):\n",
    "            self_params.data = self_params.data * beta + src_params.data * (1 - beta)\n",
    "\n",
    "\n",
    "class Paella(nn.Module):\n",
    "    def __init__(self, c_in=256, c_out=256, num_labels=8192, c_r=64, patch_size=2, c_cond=1024,\n",
    "                 c_hidden=[640, 1280, 1280], nhead=[-1, 16, 16], blocks=[4, 12, 4], level_config=['CT', 'CTA', 'CTA'],\n",
    "                 effnet_embd=16, byt5_embd=1536, kernel_size=3, dropout=0.1, self_attn=True):\n",
    "        super().__init__()\n",
    "        self.c_r = c_r\n",
    "        self.c_cond = c_cond\n",
    "        self.num_labels = num_labels\n",
    "        if not isinstance(dropout, list):\n",
    "            dropout = [dropout] * len(c_hidden)\n",
    "\n",
    "        # CONDITIONING\n",
    "        self.byt5_mapper = nn.Linear(byt5_embd, c_cond)\n",
    "        self.effnet_mapper = nn.Linear(effnet_embd, c_cond)\n",
    "        self.seq_norm = nn.LayerNorm(c_cond, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        self.in_mapper = nn.Sequential(\n",
    "            nn.Embedding(num_labels, c_in),\n",
    "            nn.LayerNorm(c_in, elementwise_affine=False, eps=1e-6)\n",
    "        )\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.PixelUnshuffle(patch_size),\n",
    "            nn.Conv2d(c_in * (patch_size ** 2), c_hidden[0], kernel_size=1),\n",
    "            LayerNorm2d(c_hidden[0], elementwise_affine=False, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        def get_block(block_type, c_hidden, nhead, c_skip=0, dropout=0):\n",
    "            if block_type == 'C':\n",
    "                return ResBlock(c_hidden, c_skip, kernel_size=kernel_size, dropout=dropout)\n",
    "            elif block_type == 'A':\n",
    "                return AttnBlock(c_hidden, c_cond, nhead, self_attn=self_attn, dropout=dropout)\n",
    "            elif block_type == 'F':\n",
    "                return FeedForwardBlock(c_hidden, dropout=dropout)\n",
    "            elif block_type == 'T':\n",
    "                return TimestepBlock(c_hidden, c_r)\n",
    "            else:\n",
    "                raise Exception(f'Block type {block_type} not supported')\n",
    "\n",
    "        # BLOCKS\n",
    "        # -- down blocks\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        for i in range(len(c_hidden)):\n",
    "            down_block = nn.ModuleList()\n",
    "            if i > 0:\n",
    "                down_block.append(nn.Sequential(\n",
    "                    LayerNorm2d(c_hidden[i - 1], elementwise_affine=False, eps=1e-6),\n",
    "                    nn.Conv2d(c_hidden[i - 1], c_hidden[i], kernel_size=2, stride=2),\n",
    "                ))\n",
    "            for _ in range(blocks[i]):\n",
    "                for block_type in level_config[i]:\n",
    "                    down_block.append(get_block(block_type, c_hidden[i], nhead[i], dropout=dropout[i]))\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # -- up blocks\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i in reversed(range(len(c_hidden))):\n",
    "            up_block = nn.ModuleList()\n",
    "            for j in range(blocks[i]):\n",
    "                for k, block_type in enumerate(level_config[i]):\n",
    "                    up_block.append(get_block(block_type, c_hidden[i], nhead[i],\n",
    "                                              c_skip=c_hidden[i] if i < len(c_hidden) - 1 and j == k == 0 else 0,\n",
    "                                              dropout=dropout[i]))\n",
    "            if i > 0:\n",
    "                up_block.append(nn.Sequential(\n",
    "                    LayerNorm2d(c_hidden[i], elementwise_affine=False, eps=1e-6),\n",
    "                    nn.ConvTranspose2d(c_hidden[i], c_hidden[i - 1], kernel_size=2, stride=2),\n",
    "                ))\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # OUTPUT\n",
    "        self.clf = nn.Sequential(\n",
    "            LayerNorm2d(c_hidden[0], elementwise_affine=False, eps=1e-6),\n",
    "            nn.Conv2d(c_hidden[0], c_out * (patch_size ** 2), kernel_size=1),\n",
    "            nn.PixelShuffle(patch_size),\n",
    "        )\n",
    "        self.out_mapper = nn.Sequential(\n",
    "            LayerNorm2d(c_out, elementwise_affine=False, eps=1e-6),\n",
    "            nn.Conv2d(c_out, num_labels, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "        # --- WEIGHT INIT ---\n",
    "        self.apply(self._init_weights)  # General init\n",
    "\n",
    "        nn.init.normal_(self.byt5_mapper.weight, std=0.02)  # conditionings\n",
    "        nn.init.normal_(self.effnet_mapper.weight, std=0.02)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding[1].weight, 0.02)  # inputs\n",
    "        nn.init.constant_(self.clf[1].weight, 0)  # outputs\n",
    "        nn.init.normal_(self.in_mapper[0].weight, std=np.sqrt(1 / num_labels))  # out mapper\n",
    "        self.out_mapper[-1].weight.data = self.in_mapper[0].weight.data[:, :, None, None].clone()\n",
    "\n",
    "        # blocks\n",
    "        for level_block in self.down_blocks + self.up_blocks:\n",
    "            for block in level_block:\n",
    "                if isinstance(block, ResBlock) or isinstance(block, FeedForwardBlock):\n",
    "                    block.channelwise[-1].weight.data *= np.sqrt(1 / sum(blocks))\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    nn.init.constant_(block.mapper.weight, 0)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def gen_r_embedding(self, r, max_positions=10000):\n",
    "        r = r * max_positions\n",
    "        half_dim = self.c_r // 2\n",
    "        emb = math.log(max_positions) / (half_dim - 1)\n",
    "        emb = torch.arange(half_dim, device=r.device).float().mul(-emb).exp()\n",
    "        emb = r[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "        if self.c_r % 2 == 1:  # zero pad\n",
    "            emb = nn.functional.pad(emb, (0, 1), mode='constant')\n",
    "        return emb\n",
    "\n",
    "    def gen_c_embeddings(self, effnet, byt5):\n",
    "        effnet = effnet.permute(0, 2, 3, 1).view(effnet.size(0), -1, effnet.size(1))\n",
    "        seq = self.effnet_mapper(effnet)\n",
    "\n",
    "        if byt5 is not None:\n",
    "            byt5 = self.byt5_mapper(byt5)\n",
    "            seq = torch.cat([seq, byt5], dim=1)\n",
    "\n",
    "        seq = self.seq_norm(seq)\n",
    "        return seq\n",
    "\n",
    "    def _down_encode(self, x, r_embed, c_embed):\n",
    "        level_outputs = []\n",
    "        for down_block in self.down_blocks:\n",
    "            for block in down_block:\n",
    "                if isinstance(block, ResBlock):\n",
    "                    x = block(x)\n",
    "                elif isinstance(block, AttnBlock):\n",
    "                    x = block(x, c_embed)\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    x = block(x, r_embed)\n",
    "                else:\n",
    "                    x = block(x)\n",
    "            level_outputs.insert(0, x)\n",
    "        return level_outputs\n",
    "\n",
    "    def _up_decode(self, level_outputs, r_embed, c_embed):\n",
    "        x = level_outputs[0]\n",
    "        for i, up_block in enumerate(self.up_blocks):\n",
    "            for j, block in enumerate(up_block):\n",
    "                if isinstance(block, ResBlock):\n",
    "                    x = block(x, level_outputs[i] if j == 0 and i > 0 else None)\n",
    "                elif isinstance(block, AttnBlock):\n",
    "                    x = block(x, c_embed)\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    x = block(x, r_embed)\n",
    "                else:\n",
    "                    x = block(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, r, effnet, byt5, x_cat=None):\n",
    "        if x_cat is not None:\n",
    "            x = torch.cat([x, x_cat], dim=1)\n",
    "        # Process the conditioning embeddings\n",
    "        r_embed = self.gen_r_embedding(r)\n",
    "        c_embed = self.gen_c_embeddings(effnet, byt5)\n",
    "\n",
    "        # Model Blocks\n",
    "        x = self.embedding(self.in_mapper(x).permute(0, 3, 1, 2))\n",
    "        level_outputs = self._down_encode(x, r_embed, c_embed)\n",
    "        x = self._up_decode(level_outputs, r_embed, c_embed)\n",
    "        x = self.out_mapper(self.clf(x))\n",
    "        return x\n",
    "\n",
    "    def add_noise(self, x, t, mask=None, random_x=None):\n",
    "        if mask is None:\n",
    "            mask = (torch.rand_like(x.float()) <= t[:, None, None]).long()\n",
    "        if random_x is None:\n",
    "            random_x = torch.randint_like(x, 0, self.num_labels)\n",
    "        x = x * (1 - mask) + random_x * mask\n",
    "        return x, mask\n",
    "\n",
    "    def get_loss_weight(self, t, mask, min_val=0.3):\n",
    "        return 1 - (1 - mask) * ((1 - t) * (1 - min_val))[:, None, None]\n",
    "\n",
    "    def update_weights_ema(self, src_model, beta=0.999):\n",
    "        for self_params, src_params in zip(self.parameters(), src_model.parameters()):\n",
    "            self_params.data = self_params.data * beta + src_params.data * (1 - beta)\n",
    "\n",
    "\n",
    "def sample(model, model_inputs, latent_shape, unconditional_inputs=None, init_x=None, steps=12, renoise_steps=None, temperature = (0.7, 0.3), cfg=(8.0, 8.0), mode = 'multinomial', t_start=1.0, t_end=0.0, sampling_conditional_steps=None, sampling_quant_steps=None, attn_weights=None): # 'quant', 'multinomial', 'argmax'\n",
    "    device = unconditional_inputs[\"byt5\"].device\n",
    "    if sampling_conditional_steps is None:\n",
    "        sampling_conditional_steps = steps\n",
    "    if sampling_quant_steps is None:\n",
    "        sampling_quant_steps = steps\n",
    "    if renoise_steps is None:\n",
    "        renoise_steps = steps-1\n",
    "    if unconditional_inputs is None:\n",
    "        unconditional_inputs = {k: torch.zeros_like(v) for k, v in model_inputs.items()}\n",
    "    intermediate_images = []\n",
    "    # with torch.inference_mode():\n",
    "    init_noise = torch.randint(0, model.num_labels, size=latent_shape, device=device)\n",
    "    if init_x != None:\n",
    "        sampled = init_x\n",
    "    else:\n",
    "        sampled = init_noise.clone()\n",
    "    t_list = torch.linspace(t_start, t_end, steps+1)\n",
    "    temperatures = torch.linspace(temperature[0], temperature[1], steps)\n",
    "    cfgs = torch.linspace(cfg[0], cfg[1], steps)\n",
    "    if cfg is not None:\n",
    "        model_inputs = {k:torch.cat([v, v_u]) for (k, v), (k_u, v_u) in zip(model_inputs.items(), unconditional_inputs.items())}\n",
    "    for i, tv in enumerate(t_list[:steps]):\n",
    "        if i >= sampling_quant_steps:\n",
    "            mode = \"quant\"\n",
    "        t = torch.ones(latent_shape[0], device=device) * tv\n",
    "\n",
    "        if cfg is not None and i < sampling_conditional_steps:\n",
    "            logits, uncond_logits = model(torch.cat([sampled]*2), torch.cat([t]*2), **model_inputs).chunk(2)\n",
    "            logits = logits * cfgs[i] + uncond_logits * (1-cfgs[i])\n",
    "        else:\n",
    "            logits = model(sampled, t, **model_inputs)\n",
    "\n",
    "        scores = logits.div(temperatures[i]).softmax(dim=1)\n",
    "\n",
    "        if mode == 'argmax':\n",
    "            sampled = logits.argmax(dim=1)\n",
    "        elif mode == 'multinomial':\n",
    "            sampled = scores.permute(0, 2, 3, 1).reshape(-1, logits.size(1))\n",
    "            sampled = torch.multinomial(sampled, 1)[:, 0].view(logits.size(0), *logits.shape[2:])\n",
    "        elif mode == 'quant':\n",
    "            sampled = scores.permute(0, 2, 3, 1) @ vqmodel.vquantizer.codebook.weight.data\n",
    "            sampled = vqmodel.vquantizer.forward(sampled, dim=-1)[-1]\n",
    "        else:\n",
    "            raise Exception(f\"Mode '{mode}' not supported, use: 'quant', 'multinomial' or 'argmax'\")\n",
    "\n",
    "        intermediate_images.append(sampled)\n",
    "\n",
    "        if i < renoise_steps:\n",
    "            t_next = torch.ones(latent_shape[0], device=device) * t_list[i+1]\n",
    "            sampled = model.add_noise(sampled, t_next, random_x=init_noise)[0]\n",
    "            intermediate_images.append(sampled)\n",
    "    return sampled, intermediate_images\n",
    "\n",
    "\n",
    "class EfficientNetEncoder(nn.Module):\n",
    "    def __init__(self, c_latent=16, effnet=\"efficientnet_v2_s\"):\n",
    "        super().__init__()\n",
    "        if effnet == \"efficientnet_v2_s\":\n",
    "            self.backbone = torchvision.models.efficientnet_v2_s(weights='DEFAULT').features.eval()\n",
    "        else:\n",
    "            print(\"Using EffNet L.\")\n",
    "            self.backbone = torchvision.models.efficientnet_v2_l(weights='DEFAULT').features.eval()\n",
    "        self.mapper = nn.Sequential(\n",
    "            nn.Conv2d(1280, c_latent, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(c_latent),  # then normalize them to have mean 0 and std 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mapper(self.backbone(x)).add(1.).div(42.)\n",
    "\n",
    "\n",
    "class Wrapper(nn.Module):\n",
    "    def __init__(self, effnet, generator, device=None):\n",
    "        super().__init__()\n",
    "        self.effnet = effnet\n",
    "        self.generator = generator\n",
    "        self.diffuzz = Diffuzz(device=device)\n",
    "\n",
    "    def forward(self, x, r, effnet_x, byt5, x_cat=None):\n",
    "        effnet = self.effnet(effnet_x)\n",
    "        if np.random.rand() < 0.05:\n",
    "            effnet = 0 * effnet\n",
    "        elif np.random.rand() < 0.3:\n",
    "            effnet = self.noise_effnet_embeds(effnet)\n",
    "        return self.generator(x, r, effnet, byt5, x_cat)\n",
    "\n",
    "    def noise_effnet_embeds(self, effnet):\n",
    "        noise_t = torch.rand(effnet.size(0), device=effnet.device) * 0.5\n",
    "        effnet = self.diffuzz.diffuse(effnet, noise_t)[0]\n",
    "        return effnet\n",
    "\n",
    "    def update_weights_ema(self, src_model, beta=0.999):\n",
    "        for self_params, src_params in zip(self.parameters(), src_model.parameters()):\n",
    "            self_params.data = self_params.data * beta + src_params.data * (1 - beta)\n",
    "\n",
    "\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self, c_in=16, c=1280, c_cond=1024, c_r=64, depth=16, nhead=16, latent_size=(12, 12), dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.c_r = c_r\n",
    "        self.projection = nn.Conv2d(c_in, c, kernel_size=1)\n",
    "        self.cond_mapper = nn.Sequential(\n",
    "            nn.Linear(c_cond, c),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(c, c),\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(depth):\n",
    "            self.blocks.append(ResBlock(c, dropout=dropout))\n",
    "            self.blocks.append(TimestepBlock(c, c_r))\n",
    "            self.blocks.append(AttnBlock(c, c, nhead, self_attn=True, dropout=dropout))\n",
    "        self.out = nn.Sequential(\n",
    "            LayerNorm2d(c, elementwise_affine=False, eps=1e-6),\n",
    "            nn.Conv2d(c, c_in * 2, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)  # General init\n",
    "        nn.init.normal_(self.projection.weight, std=0.02)  # inputs\n",
    "        nn.init.normal_(self.cond_mapper[0].weight, std=0.02)  # conditionings\n",
    "        nn.init.normal_(self.cond_mapper[-1].weight, std=0.02)  # conditionings\n",
    "        nn.init.constant_(self.out[1].weight, 0)  # outputs\n",
    "\n",
    "        # blocks\n",
    "        for block in self.blocks:\n",
    "            if isinstance(block, ResBlock):\n",
    "                block.channelwise[-1].weight.data *= np.sqrt(1 / depth)\n",
    "            elif isinstance(block, TimestepBlock):\n",
    "                nn.init.constant_(block.mapper.weight, 0)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def gen_r_embedding(self, r, max_positions=10000):\n",
    "        r = r * max_positions\n",
    "        half_dim = self.c_r // 2\n",
    "        emb = math.log(max_positions) / (half_dim - 1)\n",
    "        emb = torch.arange(half_dim, device=r.device).float().mul(-emb).exp()\n",
    "        emb = r[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "        if self.c_r % 2 == 1:  # zero pad\n",
    "            emb = nn.functional.pad(emb, (0, 1), mode='constant')\n",
    "        return emb\n",
    "\n",
    "    def forward(self, x, r, c, directionVector=None, originalAttentions=None):\n",
    "        attentions=[]\n",
    "        x_in = x\n",
    "        x = self.projection(x)\n",
    "        c_embed = self.cond_mapper(c)\n",
    "        r_embed = self.gen_r_embedding(r)\n",
    "        index=0\n",
    "        for block in self.blocks:\n",
    "            if isinstance(block, AttnBlock):\n",
    "                x = block(x, c_embed)\n",
    "                if directionVector is not None:\n",
    "                    x =x+directionVector*direction_vector_scale\n",
    "                if originalAttentions is not None:\n",
    "                    deviation = torch.abs(x - originalAttentions[index])\n",
    "                    mask = (deviation < attention_threshold).float()#threshold=0.1\n",
    "                    x = x * mask + originalAttentions[index] * (1 - mask)\n",
    "                attentions.append(x)\n",
    "                index+=1\n",
    "            elif isinstance(block, TimestepBlock):\n",
    "                x = block(x, r_embed)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        a, b = self.out(x).chunk(2, dim=1)\n",
    "        # denoised = a / (1-(1-b).pow(2)).sqrt()\n",
    "        return (x_in - a) / ((1 - b).abs() + 1e-5), attentions\n",
    "\n",
    "    def update_weights_ema(self, src_model, beta=0.999):\n",
    "        for self_params, src_params in zip(self.parameters(), src_model.parameters()):\n",
    "            self_params.data = self_params.data * beta + src_params.data * (1 - beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e647dce-5cd0-48cf-91af-e809debfc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torchtools.nn import VectorQuantize\n",
    "\n",
    "class VQResBlock(nn.Module):\n",
    "    def __init__(self, c, c_hidden):\n",
    "        super().__init__()\n",
    "        # depthwise/attention\n",
    "        self.norm1 = nn.LayerNorm(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.ReplicationPad2d(1),\n",
    "            nn.Conv2d(c, c, kernel_size=3, groups=c)\n",
    "        )\n",
    "\n",
    "        # channelwise\n",
    "        self.norm2 = nn.LayerNorm(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.channelwise = nn.Sequential(\n",
    "            nn.Linear(c, c_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(c_hidden, c),\n",
    "        )\n",
    "        \n",
    "        self.gammas = nn.Parameter(torch.zeros(6),  requires_grad=True)\n",
    "        \n",
    "        # Init weights\n",
    "        def _basic_init(module):\n",
    "            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "        self.apply(_basic_init)\n",
    "\n",
    "\n",
    "    def _norm(self, x, norm):\n",
    "        return norm(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mods = self.gammas\n",
    "            \n",
    "        x_temp = self._norm(x, self.norm1) * (1 + mods[0]) + mods[1]\n",
    "        x = x + self.depthwise(x_temp) * mods[2]\n",
    "\n",
    "        x_temp = self._norm(x, self.norm2) * (1 + mods[3]) + mods[4]\n",
    "        x = x + self.channelwise(x_temp.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) * mods[5]\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VQModel(nn.Module):\n",
    "    def __init__(self, levels=2, bottleneck_blocks=12, c_hidden=384, c_latent=4, codebook_size=8192, scale_factor=0.3764): # 1.0\n",
    "        super().__init__()\n",
    "        self.c_latent = c_latent\n",
    "        self.scale_factor = scale_factor\n",
    "        c_levels = [c_hidden//(2**i) for i in reversed(range(levels))]\n",
    "        \n",
    "        # Encoder blocks\n",
    "        self.in_block = nn.Sequential(\n",
    "            nn.PixelUnshuffle(2),\n",
    "            nn.Conv2d(3*4, c_levels[0], kernel_size=1)\n",
    "        )\n",
    "        down_blocks = []\n",
    "        for i in range(levels):\n",
    "            if i > 0:\n",
    "                down_blocks.append(nn.Conv2d(c_levels[i-1], c_levels[i], kernel_size=4, stride=2, padding=1))\n",
    "            block = VQResBlock(c_levels[i], c_levels[i]*4)\n",
    "            down_blocks.append(block)\n",
    "        down_blocks.append(nn.Sequential(\n",
    "            nn.Conv2d(c_levels[-1], c_latent, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(c_latent), # then normalize them to have mean 0 and std 1\n",
    "        ))\n",
    "        self.down_blocks = nn.Sequential(*down_blocks)\n",
    "        self.down_blocks[0]\n",
    "        \n",
    "        self.codebook_size = codebook_size\n",
    "        self.vquantizer = VectorQuantize(c_latent, k=codebook_size)        \n",
    "\n",
    "        # Decoder blocks\n",
    "        up_blocks = [nn.Sequential(\n",
    "            nn.Conv2d(c_latent, c_levels[-1], kernel_size=1)\n",
    "        )]\n",
    "        for i in range(levels):\n",
    "            for j in range(bottleneck_blocks if i == 0 else 1):\n",
    "                block = VQResBlock(c_levels[levels-1-i], c_levels[levels-1-i]*4)\n",
    "                up_blocks.append(block)\n",
    "            if i < levels-1:\n",
    "                up_blocks.append(nn.ConvTranspose2d(c_levels[levels-1-i], c_levels[levels-2-i], kernel_size=4, stride=2, padding=1))\n",
    "        self.up_blocks = nn.Sequential(*up_blocks)\n",
    "        self.out_block = nn.Sequential(\n",
    "            nn.Conv2d(c_levels[0], 3*4, kernel_size=1),\n",
    "            nn.PixelShuffle(2),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.in_block(x)\n",
    "        x = self.down_blocks(x)\n",
    "        qe, (vq_loss, commit_loss), indices = self.vquantizer.forward(x, dim=1)\n",
    "        return qe / self.scale_factor, x / self.scale_factor, indices, vq_loss + commit_loss * 0.25\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = x * self.scale_factor\n",
    "        x = self.up_blocks(x)\n",
    "        x = self.out_block(x)\n",
    "        return x\n",
    "\n",
    "    def decode_indices(self, x):\n",
    "        x = self.vquantizer.idx2vq(x, dim=1)\n",
    "        x = self.up_blocks(x)\n",
    "        x = self.out_block(x)\n",
    "        return x\n",
    "\n",
    "    def encode_img(self, x):\n",
    "        x = self.in_block(x)\n",
    "        x = self.down_blocks(x)\n",
    "        qe, (vq_loss, commit_loss), indices = self.vquantizer.forward(x, dim=1)\n",
    "        return qe\n",
    "        \n",
    "    def forward(self, x, quantize=False):\n",
    "        qe, x, _, vq_loss = self.encode(x, quantize)\n",
    "        x = self.decode(qe)\n",
    "        return x, vq_loss\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, c_in=3, c_cond=0, c_hidden=512, depth=6):\n",
    "        super().__init__()\n",
    "        d = max(depth - 3, 3)\n",
    "        layers = [\n",
    "            nn.utils.spectral_norm(nn.Conv2d(c_in, c_hidden // (2 ** d), kernel_size=3, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        ]\n",
    "        for i in range(depth - 1):\n",
    "            c_in = c_hidden // (2 ** max((d - i), 0))\n",
    "            c_out = c_hidden // (2 ** max((d - 1 - i), 0))\n",
    "            layers.append(nn.utils.spectral_norm(nn.Conv2d(c_in, c_out, kernel_size=3, stride=2, padding=1)))\n",
    "            layers.append(nn.InstanceNorm2d(c_out))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.shuffle = nn.Conv2d((c_hidden + c_cond) if c_cond > 0 else c_hidden, 1, kernel_size=1)\n",
    "        self.logits = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, cond=None):\n",
    "        x = self.encoder(x)\n",
    "        if cond is not None:\n",
    "            cond = cond.view(cond.size(0), cond.size(1), 1, 1, ).expand(-1, -1, x.size(-2), x.size(-1))\n",
    "            x = torch.cat([x, cond], dim=1)\n",
    "        x = self.shuffle(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373230e1-945f-4b09-bfe8-558a105f5720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7a16a79de14b88adebe04e2b1d350e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7722cee1de574322bb0d88e3f3c7107a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/120k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db28a7a135a47189f537c6091768cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4d3a5a2ccb4af999d3e2ba82481461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1001b7083bf45ab820a389ba8141cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/169M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39524c6ffa004daaaa1a9f543324f642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd73c1453d84b278a8ab3b0eeefd24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/904 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03170de5ea4c46158b2b0eb43570de2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06f4edde987469ba619dde920f6c2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102db824c6c5498a90dbc8cf5c64cd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd0a90ae96d484e85fd9d744275c281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8437267b5ea042e19e19725c3db49dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c83059b19544dbba7d396db43b05e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5613af01c2224d73a909425363ff46f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/904 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c004e1b3ade44aeb5178f27759fa739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63ba39f02984ddfb6f37af1c71ff310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a02d68bd1464513806a8718a14e5a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb1c120c709434cac6f22fd96c596a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n",
      "100%|██████████| 82.7M/82.7M [00:00<00:00, 102MB/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, T5EncoderModel, CLIPTextModel\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "if 'diffuzz' in globals():\n",
    "    del diffuzz\n",
    "\n",
    "if 'effnet' in globals():\n",
    "    del effnet\n",
    "    \n",
    "if 'generator' in globals():\n",
    "    del generator\n",
    "    \n",
    "if 'model' in globals():\n",
    "    del model\n",
    "    \n",
    "checkpoint_stage_a = \"/workspace/models/vqgan_f4_v1_500k.pt\"\n",
    "checkpoint_stage_b = \"/workspace/models/model_v2_stage_b.pt\"\n",
    "checkpoint_stage_c = \"/workspace/models/model_v2_stage_c_finetune_interpolation.pt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "effnet_preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(768, interpolation=torchvision.transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    torchvision.transforms.CenterCrop(768),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "def encode(vqmodel, img_seq):\n",
    "    return vqmodel.encode_img(img_seq)\n",
    "\n",
    "def decode(img_seq):\n",
    "    return vqmodel.decode(img_seq)\n",
    "\n",
    "def embed_clip(clip_tokenizer, clip_model, caption, negative_caption=\"\", batch_size=4, device=\"cuda\"):\n",
    "    clip_tokens = clip_tokenizer([caption] * batch_size, truncation=True, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\").to(device)\n",
    "    clip_text_embeddings = clip_model(**clip_tokens).last_hidden_state\n",
    "\n",
    "    clip_tokens_uncond = clip_tokenizer([negative_caption] * batch_size, truncation=True, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\").to(device)\n",
    "    clip_text_embeddings_uncond = clip_model(**clip_tokens_uncond).last_hidden_state\n",
    "    return clip_text_embeddings, clip_text_embeddings_uncond\n",
    "\n",
    "vqmodel = VQModel().to(device)\n",
    "vqmodel.load_state_dict(torch.load(checkpoint_stage_a, map_location=device)[\"state_dict\"])\n",
    "vqmodel.eval().requires_grad_(False)\n",
    "\n",
    "clip_model = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\").to(device).eval().requires_grad_(False)\n",
    "clip_tokenizer = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\")\n",
    "\n",
    "clip_model_b = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\").eval().requires_grad_(False).to(device)\n",
    "clip_tokenizer_b = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "\n",
    "diffuzz = Diffuzz(device=device)\n",
    "\n",
    "pretrained_checkpoint = torch.load(checkpoint_stage_b, map_location=device)\n",
    "\n",
    "effnet = EfficientNetEncoder().to(device)\n",
    "effnet.load_state_dict(pretrained_checkpoint['effnet_state_dict'])\n",
    "effnet.eval().requires_grad_(False)\n",
    "\n",
    "# - LDM Model as generator -\n",
    "generator = DiffNeXt()\n",
    "generator.load_state_dict(pretrained_checkpoint['state_dict'])\n",
    "generator.eval().requires_grad_(False).to(device)\n",
    "\n",
    "del pretrained_checkpoint\n",
    "\n",
    "checkpoint = torch.load(checkpoint_stage_c, map_location=device)\n",
    "model = Prior(c_in=16, c=1536, c_cond=1280, c_r=64, depth=32, nhead=24).to(device)\n",
    "model.load_state_dict(checkpoint['ema_state_dict'])\n",
    "model.eval().requires_grad_(False)\n",
    "del checkpoint\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d591f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_clip_embeddings(clip_tokenizer, clip_model, sentences, batch_size, device):\n",
    "    all_embeddings = []\n",
    "    all_embeddings_uncond = []\n",
    "    for sentence in sentences:\n",
    "        embeddings, embeddings_uncond = embed_clip(clip_tokenizer, clip_model, sentence, batch_size=batch_size, device=device)\n",
    "        all_embeddings.append(embeddings.mean(dim=0))\n",
    "        all_embeddings_uncond.append(embeddings_uncond.mean(dim=0))\n",
    "    return torch.stack(all_embeddings).mean(dim=0), torch.stack(all_embeddings_uncond).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20172725-e2eb-425b-bcad-13173d9656ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 1280])\n",
      "torch.Size([1, 16, 24, 24])\n",
      "encoded shape  torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 16, 24, 24])\n",
      "torch.Size([1, 16, 24, 24])\n",
      "torch.Size([1, 16, 24, 24])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 2 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 134\u001b[0m\n\u001b[1;32m    132\u001b[0m curIndex\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_end, steps \u001b[38;5;129;01min\u001b[39;00m prior_inference_steps\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 134\u001b[0m     sampled, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mdiffuzz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_snowy_weather_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdirectionVector\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginalAttentions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginalAttentions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurIndex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munconditional_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_snowy_weather_embeddings_uncond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior_features_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtargetX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshrinkedImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetAlphas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurSteps\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotalSteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurSteps\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotalSteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.28\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mt_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     sampled \u001b[38;5;241m=\u001b[39m sampled[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    138\u001b[0m     curIndex \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 129\u001b[0m, in \u001b[0;36mDiffuzz.sample\u001b[0;34m(self, model, model_inputs, shape, mask, t_start, t_end, timesteps, x_init, cfg, unconditional_inputs, sampler, half, targetX, targetAlphas)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Prior):\n\u001b[0;32m--> 129\u001b[0m         pred_noise, attention \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_range\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodified_model_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         pred_noise \u001b[38;5;241m=\u001b[39m model(x, r_range[i], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodified_model_inputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 657\u001b[0m, in \u001b[0;36mPrior.forward\u001b[0;34m(self, x, r, c, directionVector, originalAttentions)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(block, AttnBlock):\n\u001b[0;32m--> 657\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m directionVector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m             x \u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m+\u001b[39mdirectionVector\u001b[38;5;241m*\u001b[39mdirection_vector_scale\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mAttnBlock.forward\u001b[0;34m(self, x, kv)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, kv):\n\u001b[1;32m     40\u001b[0m     kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_mapper(kv)\n\u001b[0;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 81\u001b[0m, in \u001b[0;36mAttention2D.forward\u001b[0;34m(self, x, kv, self_attn)\u001b[0m\n\u001b[1;32m     79\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Bx4xHxW -> Bx(HxW)x4\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_attn:\n\u001b[0;32m---> 81\u001b[0m     kv \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x, kv, kv, need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     83\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39morig_shape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 2 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import requests\n",
    "batch_size = 1\n",
    "caption = \"A snowy landscape\"\n",
    "negative_caption = \"\"\n",
    "prior_inference_steps = {2/3: 20, 0.0: 10}\n",
    "prior_cfg = 4\n",
    "prior_sampler = \"ddim\"\n",
    "\n",
    "direction_vector_scale = 0.1\n",
    "attention_threshold = 10\n",
    "\n",
    "generator_steps = 30\n",
    "generator_cfg = None\n",
    "generator_sampler = \"ddim\"\n",
    "\n",
    "height = 1024\n",
    "width = 1024\n",
    "\n",
    "effnet_features_shape = (batch_size, 16, 12, 12)\n",
    "\n",
    "clip_text_embeddings, clip_text_embeddings_uncond = embed_clip(clip_tokenizer, clip_model, caption, negative_caption, batch_size, device)\n",
    "print(clip_text_embeddings.size())\n",
    "\n",
    "\n",
    "latent_height = 128 * (height // 128) // (1024 // 24)\n",
    "latent_width = 128 * (width // 128) // (1024 // 24)\n",
    "prior_features_shape = (batch_size, 16, latent_height, latent_width)\n",
    "effnet_embeddings_uncond = torch.zeros(effnet_features_shape).to(device)\n",
    "generator_latent_shape = (batch_size, 4, int(latent_height * (256 / 24)), int(latent_width * (256 / 24)))\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "effnet_preprocess = transforms.Compose([\n",
    "    transforms.Resize(768, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    #transforms.Resize(384, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    #transforms.CenterCrop(384),\n",
    "    transforms.CenterCrop(768),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406), \n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "response = requests.get(\"https://upload.wikimedia.org/wikipedia/commons/3/35/Neckertal_20150527-6384.jpg\", stream=True)\n",
    "image = Image.open(response.raw)\n",
    "transformedImage = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(1024),\n",
    "    torchvision.transforms.CenterCrop(1024),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "good_weather_sentences = [\n",
    "    \"The tranquil scene of a calm lake reflecting the clear, sunny skies, surrounded by vibrant flowers.\",\n",
    "    \"A panoramic view of rolling hills basking in the golden glow of a perfect sunny day.\",\n",
    "    \"A serene countryside landscape illuminated by the gentle light of a pleasant, sunny afternoon.\",\n",
    "    \"The majestic view of mountains under a clear blue sky, with the sun shining brightly.\",\n",
    "    \"A vibrant forest path drenched in sunlight, with birds chirping and a gentle breeze.\",\n",
    "    \"A picturesque coastal landscape with clear skies, the sun shining, and calm sea waves.\",\n",
    "    \"A beautiful garden full of blooming flowers and lush greenery on a bright, sunny day.\",\n",
    "    \"A scenic river gently flowing through a sunny valley, surrounded by rich, green foliage.\",\n",
    "    \"A peaceful orchard with fruit trees in full bloom under a clear, blue, sunny sky.\"\n",
    "]\n",
    "\n",
    "snowy_weather_sentences = [\n",
    "    \"A serene landscape blanketed in fresh, white snow, with snowflakes gently falling.\",\n",
    "    \"A frosty forest under a snowy sky, with trees covered in a thick layer of snow.\",\n",
    "    \"A tranquil mountain range with peaks capped in white snow under a cloudy, snowy sky.\",\n",
    "    \"A cozy village covered in snow, with smoke rising from chimneys on a cold winter day.\",\n",
    "    \"A frozen lake surrounded by snow-covered trees and a soft, wintry haze.\",\n",
    "    \"A snowy path winding through a winter forest, with snow-laden branches overhead.\",\n",
    "    \"An icy river cutting through a snow-covered landscape, with frost glistening in the light.\",\n",
    "    \"A winter wonderland scene with heavy snowfall covering the hills and fields.\",\n",
    "    \"A quiet, snow-covered meadow under the soft light of a cloudy, snowy day.\",\n",
    "    \"A panoramic view of a snow-blanketed countryside during a serene snowfall.\"\n",
    "]\n",
    "\n",
    "avg_good_weather_embeddings, avg_good_weather_embeddings_uncond = average_clip_embeddings(clip_tokenizer, clip_model, good_weather_sentences, batch_size, device)\n",
    "avg_snowy_weather_embeddings, avg_snowy_weather_embeddings_uncond = average_clip_embeddings(clip_tokenizer, clip_model, snowy_weather_sentences, batch_size, device)\n",
    "\n",
    "direction_vector = avg_snowy_weather_embeddings - avg_good_weather_embeddings\n",
    "\n",
    "if image is not None:\n",
    "    with torch.no_grad():\n",
    "        shrinkedImage = effnet((effnet_preprocess(image)).to(device).unsqueeze(0))\n",
    "        shrinkedImage= shrinkedImage.to(torch.bfloat16)\n",
    "        print(shrinkedImage.size())\n",
    "\n",
    "        encoded_image=encode(vqmodel, transformedImage(image).to(device).unsqueeze(0))\n",
    "        print(\"encoded shape \",encoded_image.size())\n",
    "\n",
    "\n",
    "with torch.cuda.amp.autocast(dtype=torch.bfloat16), torch.no_grad():\n",
    "    s = time.time()\n",
    "    t_start = 1.0\n",
    "    sampled = shrinkedImage\n",
    "\n",
    "\n",
    "    #originalSampled = []\n",
    "    originalAttentions = []\n",
    "    totalSteps = sum(prior_inference_steps.values())\n",
    "    curSteps = 0\n",
    "    for t_end, steps in prior_inference_steps.items():\n",
    "        sampled, attentions = diffuzz.sample(model, {'c': avg_good_weather_embeddings.unsqueeze(0), 'directionVector': None, 'originalAttentions': None}, x_init=sampled, unconditional_inputs={\"c\": avg_good_weather_embeddings_uncond.unsqueeze(0)}, shape=prior_features_shape,\n",
    "                            timesteps=steps, cfg=0, sampler=prior_sampler, targetX=shrinkedImage, targetAlphas = [(curSteps + i) / totalSteps for i in range(steps + 1)],\n",
    "                            t_start=t_start, t_end=t_end)\n",
    "        sampled = sampled[-1]\n",
    "\n",
    "        #originalSampled.extend(sampled)\n",
    "        originalAttentions.append(attentions)\n",
    "        curSteps +=steps\n",
    "        t_start = t_end\n",
    "\n",
    "    sampled = None\n",
    "#\n",
    "    for j in range(3):\n",
    "        curSteps = 0\n",
    "        curIndex = 0\n",
    "        t_start = 1.0\n",
    "        curIndex=0\n",
    "        for t_end, steps in prior_inference_steps.items():\n",
    "            sampled, attentions = diffuzz.sample(model, {'c': avg_snowy_weather_embeddings.unsqueeze(0), 'directionVector': None, 'originalAttentions': originalAttentions[curIndex]}, x_init=sampled, unconditional_inputs={\"c\": avg_snowy_weather_embeddings_uncond.unsqueeze(0)}, shape=prior_features_shape,\n",
    "                                timesteps=steps, cfg=generator_cfg, sampler=prior_sampler,targetX=shrinkedImage, targetAlphas = [(curSteps+i) / totalSteps if (curSteps+i) / totalSteps <= 0.28 else 0 for i in range(steps + 1)],\n",
    "                                t_start=t_start, t_end=t_end)\n",
    "            sampled = sampled[-1]\n",
    "            curIndex +=1\n",
    "            t_start = t_end\n",
    "            curSteps+=steps\n",
    "\n",
    "    sampled = sampled.mul(42).sub(1)\n",
    "    shrinkedImage = shrinkedImage.mul(42).sub(1)\n",
    "\n",
    "    print(f\"Prior Sampling: {time.time() - s}\")\n",
    "\n",
    "    clip_text_embeddings, clip_text_embeddings_uncond = average_clip_embeddings(clip_tokenizer_b, clip_model_b, snowy_weather_sentences, batch_size, device)\n",
    "\n",
    "    s = time.time()\n",
    "    sampled_images_original = None\n",
    "    for i in range(1):\n",
    "        sampled_images_original, _ = diffuzz.sample(generator, {'effnet': shrinkedImage, 'clip': clip_text_embeddings.unsqueeze(0)},\n",
    "                                generator_latent_shape, t_start=1.0, t_end=0.00, x_init=sampled_images_original,\n",
    "                                timesteps=generator_steps, cfg=generator_cfg, sampler=generator_sampler,\n",
    "                                unconditional_inputs = {\n",
    "                                'effnet': effnet_embeddings_uncond, 'clip': clip_text_embeddings_uncond.unsqueeze(0),\n",
    "                            })\n",
    "        sampled_images_original = sampled_images_original[-1]\n",
    "    print(sampled_images_original.size())\n",
    "    print(f\"Generator Sampling: {time.time() - s}\")\n",
    "\n",
    "s = time.time()\n",
    "sampled = decode(sampled_images_original)\n",
    "print(f\"Decoder Generation: {time.time() - s}\")\n",
    "print(f\"Prior => CFG: {prior_cfg}, Steps: {sum(prior_inference_steps.values())}, Sampler: {prior_sampler}\")\n",
    "print(f\"Generator => CFG: {generator_cfg}, Steps: {generator_steps}, Sampler: {generator_sampler}\")\n",
    "print(f\"Images Shape: {sampled.shape}\")\n",
    "print(caption)\n",
    "duration_seconds = time.time() - s\n",
    "print(duration_seconds)\n",
    "\n",
    "minutes = int(duration_seconds // 60)\n",
    "seconds = int(duration_seconds % 60)\n",
    "formatted_time = f\"Time taken: {minutes} minutes and {seconds} seconds\"\n",
    "\n",
    "print(formatted_time)\n",
    "for i, img in enumerate(sampled):\n",
    "    img_to_save = torch.clamp(img, 0, 1)\n",
    "    img_to_save = img_to_save.permute(1, 2, 0).cpu().numpy()\n",
    "    # Save the image \n",
    "    plt.imsave(os.path.join(\"/workspace/\", f\"img_{i}.png\"), img_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bb92a21-0e92-4632-ab3b-5bd1bd0b8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833b4cc-96dd-4a60-b313-79de52743ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c0e69-a6ae-4a40-b02f-bb9cbd36bff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef48b3-e36d-4e12-8d99-75b890209b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f97138-7c47-411b-8952-9643435206ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
