{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-04T15:12:54.534060Z","iopub.status.busy":"2023-12-04T15:12:54.533662Z","iopub.status.idle":"2023-12-04T15:12:59.596960Z","shell.execute_reply":"2023-12-04T15:12:59.596007Z","shell.execute_reply.started":"2023-12-04T15:12:54.534031Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","import numpy as np\n","import torchvision\n","from torch import nn\n","import torch\n","from torch.utils.data import DataLoader, IterableDataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:12:59.599334Z","iopub.status.busy":"2023-12-04T15:12:59.598875Z","iopub.status.idle":"2023-12-04T15:13:00.251031Z","shell.execute_reply":"2023-12-04T15:13:00.249986Z","shell.execute_reply.started":"2023-12-04T15:12:59.599307Z"},"trusted":true},"outputs":[],"source":["train_dataset = load_dataset(\"mingyy/chinese_landscape_paintings\", split='train', streaming=True)\n","class StreamingDataLoader(IterableDataset):\n","    def __init__(self, dataset, batch_size):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","\n","    def __iter__(self):\n","        batch = []\n","        for item in self.dataset:\n","            batch.append(item)\n","            if len(batch) == self.batch_size:\n","                yield batch\n","                batch = []\n","        if batch:\n","            yield batch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:00.252712Z","iopub.status.busy":"2023-12-04T15:13:00.252332Z","iopub.status.idle":"2023-12-04T15:13:00.257856Z","shell.execute_reply":"2023-12-04T15:13:00.256755Z","shell.execute_reply.started":"2023-12-04T15:13:00.252679Z"},"trusted":true},"outputs":[],"source":["def streaming_map(dataset, transform_fn):\n","    for sample in dataset:\n","        yield transform_fn(sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:00.259920Z","iopub.status.busy":"2023-12-04T15:13:00.259170Z","iopub.status.idle":"2023-12-04T15:13:00.270042Z","shell.execute_reply":"2023-12-04T15:13:00.269140Z","shell.execute_reply.started":"2023-12-04T15:13:00.259884Z"},"trusted":true},"outputs":[],"source":["\n","transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Resize(512),\n","    #torchvision.transforms.RandomCrop(512),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:00.273792Z","iopub.status.busy":"2023-12-04T15:13:00.273438Z","iopub.status.idle":"2023-12-04T15:13:00.281104Z","shell.execute_reply":"2023-12-04T15:13:00.280180Z","shell.execute_reply.started":"2023-12-04T15:13:00.273759Z"},"trusted":true},"outputs":[],"source":["batch_size=64\n","\n","\n","transformed_dataset = streaming_map(train_dataset, transforms)\n","dataloader = DataLoader(StreamingDataLoader(transformed_dataset, batch_size=batch_size), \n","                        num_workers=2, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:00.282970Z","iopub.status.busy":"2023-12-04T15:13:00.282601Z","iopub.status.idle":"2023-12-04T15:13:00.302662Z","shell.execute_reply":"2023-12-04T15:13:00.301783Z","shell.execute_reply.started":"2023-12-04T15:13:00.282941Z"},"trusted":true},"outputs":[],"source":["\n","class Encoder(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(Encoder, self).__init__()\n","        self.conv_layers = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 128, 4, 2, 1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 256, 4, 2, 1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(256, 512, 4, 2, 1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # Additional layers can be added here if needed\n","        )\n","        self.conv6_mu = nn.Conv2d(512, latent_dim, 4, 2, 1)\n","        self.conv6_logvar = nn.Conv2d(512, latent_dim, 4, 2, 1)\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)\n","        mu = self.conv6_mu(x)\n","        logvar = self.conv6_logvar(x)\n","        return mu, logvar\n","\n","class Decoder(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(Decoder, self).__init__()\n","        self.deconv_layers = nn.Sequential(\n","            nn.ConvTranspose2d(latent_dim, 512, 4, 2, 1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(64, 3, 4, 2, 1),\n","            nn.Tanh()  # Use nn.Sigmoid() if your data is normalized to [0,1]\n","        )\n","\n","    def forward(self, z):\n","        z = self.deconv_layers(z)\n","        return z\n","\n","class VAE(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(VAE, self).__init__()\n","        self.encoder = Encoder(latent_dim)\n","        self.decoder = Decoder(latent_dim)\n","\n","    def reparameterize(self, mu, logvar):\n","        logvar = torch.clamp(logvar, -30, 20)\n","        variance = logvar.exp()\n","        stdev = variance.sqrt()\n","        eps = torch.randn_like(stdev)\n","        z = mu + eps * stdev\n","        return z\n","\n","    def forward(self, x):\n","        mu, logvar = self.encoder(x)\n","        z = self.reparameterize(mu, logvar)\n","        #x_recon = self.decoder(z)\n","        return z"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:00.304107Z","iopub.status.busy":"2023-12-04T15:13:00.303804Z","iopub.status.idle":"2023-12-04T15:13:03.932545Z","shell.execute_reply":"2023-12-04T15:13:03.931620Z","shell.execute_reply.started":"2023-12-04T15:13:00.304079Z"},"trusted":true},"outputs":[],"source":["vae = VAE(128)\n","checkpoint = torch.load(\"/vae.pth\")\n","vae.load_state_dict(checkpoint['vae'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:03.933870Z","iopub.status.busy":"2023-12-04T15:13:03.933590Z","iopub.status.idle":"2023-12-04T15:13:03.938959Z","shell.execute_reply":"2023-12-04T15:13:03.938009Z","shell.execute_reply.started":"2023-12-04T15:13:03.933846Z"},"trusted":true},"outputs":[],"source":["def get_time_embedding(timestep, d_embed=64):\n","    freqs = torch.pow(10000, -torch.arange(start=0, end=d_embed, step=2, dtype=torch.float32) / d_embed)\n","    args = timestep * freqs\n","    embedding = torch.cat([torch.cos(args), torch.sin(args)])\n","    return embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:03.940342Z","iopub.status.busy":"2023-12-04T15:13:03.940047Z","iopub.status.idle":"2023-12-04T15:13:03.958093Z","shell.execute_reply":"2023-12-04T15:13:03.957365Z","shell.execute_reply.started":"2023-12-04T15:13:03.940319Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class DownsampleBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DownsampleBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x_pooled = self.pool(x)\n","        return x, x_pooled\n","\n","class UpsampleBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super(UpsampleBlock, self).__init__()\n","        if mid_channels is None:\n","            mid_channels = out_channels\n","        self.up = nn.ConvTranspose2d(in_channels // 2, out_channels, kernel_size=2, stride=2)\n","        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        x = torch.cat([x2, x1], dim=1)\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        return x\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(ConvBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        return x\n","\n","class UNetDiffusionModel(nn.Module):\n","    def __init__(self, time_embedding_channels, batch_size):\n","        super(UNetDiffusionModel, self).__init__()\n","        self.down1 = ConvBlock(128 + 128+ time_embedding_channels, 256)\n","        self.down2 = ConvBlock(256, 512)\n","        self.up1 = ConvBlock(512, 256)\n","        self.up2 = ConvBlock(256, 128)\n","        self.out_conv = nn.Conv2d(128, 128, kernel_size=1)\n","        self.time_embed = SinusoidalTimeEmbedding(time_embedding_channels)\n","        self.batch_size = batch_size\n","\n","    def forward(self, x, t, crops):\n","        time_emb = self.time_embed(t)\n","        time_emb = time_emb.unsqueeze(-1).unsqueeze(-1)\n","        time_emb = time_emb.expand(-1, -1, x.shape[2], x.shape[3])\n","        time_emb = time_emb.repeat(self.batch_size, 1, 1, 1)\n","\n","        # Concatenate time embedding with the input feature map\n","        x = torch.cat([x, time_emb, crops], dim=1)\n","\n","        # Convolutional blocks without downsampling\n","        x = self.down1(x)\n","        x = self.down2(x)\n","\n","        # Upsampling layers\n","        x = self.up1(x)\n","        x = self.up2(x)\n","\n","        # Output layer\n","        x = self.out_conv(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:03.959188Z","iopub.status.busy":"2023-12-04T15:13:03.958919Z","iopub.status.idle":"2023-12-04T15:13:03.972720Z","shell.execute_reply":"2023-12-04T15:13:03.971915Z","shell.execute_reply.started":"2023-12-04T15:13:03.959166Z"},"trusted":true},"outputs":[],"source":["class SinusoidalTimeEmbedding(nn.Module):\n","    def __init__(self, dim):\n","        super(SinusoidalTimeEmbedding, self).__init__()\n","        self.dim = dim\n","\n","    def forward(self, t):\n","        half_dim = self.dim // 2\n","        emb = math.log(10000) / (half_dim - 1)\n","        emb = (torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)).to(device)\n","        emb = t.float() * emb.unsqueeze(0)\n","        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n","        if self.dim % 2 == 1:  # Zero pad if odd dimension\n","            emb = F.pad(emb, (0, 1, 0, 0))\n","        return emb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:03.974276Z","iopub.status.busy":"2023-12-04T15:13:03.973902Z","iopub.status.idle":"2023-12-04T15:13:03.997449Z","shell.execute_reply":"2023-12-04T15:13:03.996751Z","shell.execute_reply.started":"2023-12-04T15:13:03.974245Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn.functional as F\n","\n","class DDPMSampler:\n","\n","    def __init__(self, generator: torch.Generator, num_training_steps=1000, beta_start: float = 0.00085, beta_end: float = 0.0120):\n","        # Params \"beta_start\" and \"beta_end\" taken from: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/configs/stable-diffusion/v1-inference.yaml#L5C8-L5C8\n","        # For the naming conventions, refer to the DDPM paper (https://arxiv.org/pdf/2006.11239.pdf)\n","        self.betas = torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_training_steps, dtype=torch.float32) ** 2\n","        self.alphas = 1.0 - self.betas\n","        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n","        self.one = torch.tensor(1.0)\n","\n","        self.generator = generator\n","\n","        self.num_train_timesteps = num_training_steps\n","        self.timesteps = torch.from_numpy(np.arange(0, num_training_steps)[::-1].copy())\n","\n","    def set_inference_timesteps(self, num_inference_steps=50):\n","        self.num_inference_steps = num_inference_steps\n","        step_ratio = self.num_train_timesteps // self.num_inference_steps\n","        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)\n","        self.timesteps = torch.from_numpy(timesteps)\n","\n","    def _get_previous_timestep(self, timestep: int) -> int:\n","        prev_t = timestep - self.num_train_timesteps // self.num_inference_steps\n","        return prev_t\n","    \n","    def _get_variance(self, timestep: int) -> torch.Tensor:\n","        prev_t = self._get_previous_timestep(timestep)\n","\n","        alpha_prod_t = self.alphas_cumprod[timestep]\n","        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n","        current_beta_t = 1 - alpha_prod_t / alpha_prod_t_prev\n","\n","        # For t > 0, compute predicted variance βt (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)\n","        # and sample from it to get previous sample\n","        # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample\n","        variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t\n","\n","        # we always take the log of variance, so clamp it to ensure it's not 0\n","        variance = torch.clamp(variance, min=1e-20)\n","\n","        return variance\n","    \n","    def set_strength(self, strength=1):\n","        \"\"\"\n","            Set how much noise to add to the input image. \n","            More noise (strength ~ 1) means that the output will be further from the input image.\n","            Less noise (strength ~ 0) means that the output will be closer to the input image.\n","        \"\"\"\n","        # start_step is the number of noise levels to skip\n","        start_step = self.num_inference_steps - int(self.num_inference_steps * strength)\n","        self.timesteps = self.timesteps[start_step:]\n","        self.start_step = start_step\n","\n","    def step(self, timestep: int, latents: torch.Tensor, model_output: torch.Tensor):\n","        t = timestep\n","        prev_t = self._get_previous_timestep(t)\n","\n","        # 1. compute alphas, betas\n","        alpha_prod_t = self.alphas_cumprod[t]\n","        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n","        beta_prod_t = 1 - alpha_prod_t\n","        beta_prod_t_prev = 1 - alpha_prod_t_prev\n","        current_alpha_t = alpha_prod_t / alpha_prod_t_prev\n","        current_beta_t = 1 - current_alpha_t\n","\n","        # 2. compute predicted original sample from predicted noise also called\n","        # \"predicted x_0\" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf\n","        pred_original_sample = (latents - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n","\n","        # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t\n","        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n","        pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * current_beta_t) / beta_prod_t\n","        current_sample_coeff = current_alpha_t ** (0.5) * beta_prod_t_prev / beta_prod_t\n","\n","        # 5. Compute predicted previous sample µ_t\n","        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n","        pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * latents\n","\n","        # 6. Add noise\n","        variance = 0\n","        if t > 0:\n","            device = model_output.device\n","            noise = torch.randn(model_output.shape, generator=self.generator, device=device, dtype=model_output.dtype)\n","            # Compute the variance as per formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n","            variance = (self._get_variance(t) ** 0.5) * noise\n","        \n","        # sample from N(mu, sigma) = X can be obtained by X = mu + sigma * N(0, 1)\n","        # the variable \"variance\" is already multiplied by the noise N(0, 1)\n","        pred_prev_sample = pred_prev_sample + variance\n","\n","        return pred_prev_sample\n","    \n","    def add_noise(\n","        self,\n","        original_samples: torch.FloatTensor,\n","        timesteps: torch.IntTensor,\n","    ) -> torch.FloatTensor:\n","        alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n","        timesteps = timesteps.to(original_samples.device)\n","\n","        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n","        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n","        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n","            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n","\n","        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n","        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n","        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n","            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n","\n","        # Sample from q(x_t | x_0) as in equation (4) of https://arxiv.org/pdf/2006.11239.pdf\n","        # Because N(mu, sigma) = X can be obtained by X = mu + sigma * N(0, 1)\n","        # here mu = sqrt_alpha_prod * original_samples and sigma = sqrt_one_minus_alpha_prod\n","        noise = torch.randn(original_samples.shape, generator=self.generator, device=original_samples.device, dtype=original_samples.dtype)\n","        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n","        return noisy_samples\n","\n","        \n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:03.998789Z","iopub.status.busy":"2023-12-04T15:13:03.998519Z","iopub.status.idle":"2023-12-04T15:13:04.115045Z","shell.execute_reply":"2023-12-04T15:13:04.113951Z","shell.execute_reply.started":"2023-12-04T15:13:03.998767Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","diffusion = UNetDiffusionModel(128, batch_size).to(device)\n","generator = torch.Generator(device=device)\n","sampler = DDPMSampler(generator)\n","vae.to(device)\n","\n","for parameter in vae.parameters():\n","    parameter.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:04.116708Z","iopub.status.busy":"2023-12-04T15:13:04.116283Z","iopub.status.idle":"2023-12-04T15:13:04.306299Z","shell.execute_reply":"2023-12-04T15:13:04.305306Z","shell.execute_reply.started":"2023-12-04T15:13:04.116673Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","\n","def process_image(pil_image, required_dimension=512):\n","    # Read the image\n","    numpy_image = np.array(pil_image)\n","    image_rgb = cv2.cvtColor(numpy_image, cv2.COLOR_BGR2RGB)\n","\n","    # Create a mask for white pixels\n","    lower_white = np.array([254, 254, 254], dtype=np.uint8)\n","    upper_white = np.array([255, 255, 255], dtype=np.uint8)\n","    white_pixels_mask = cv2.inRange(image_rgb, lower_white, upper_white)\n","\n","    # Invert the mask to get non-white areas\n","    non_white_pixels_mask = cv2.bitwise_not(white_pixels_mask)\n","\n","    # Use the non-white pixel mask to create the separated_rgb image\n","    separated_rgb = cv2.bitwise_and(image_rgb, image_rgb, mask=non_white_pixels_mask)\n","\n","    # Find contours in the non_white_pixels_mask\n","    contours, _ = cv2.findContours(non_white_pixels_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    if contours:\n","        x, y, w, h = cv2.boundingRect(contours[0])\n","        for contour in contours:\n","            x1, y1, w1, h1 = cv2.boundingRect(contour)\n","            x, y, w, h = min(x, x1), min(y, y1), max(x + w, x1 + w1) - min(x, x1), max(y + h, y1 + h1) - min(y, y1)\n","\n","        # Crop the separated_rgb image\n","        cropped_image = separated_rgb[y:y+h, x:x+w]\n","\n","        # Determine the smaller dimension\n","        min_dimension = min(cropped_image.shape[0], cropped_image.shape[1])\n","\n","        # Scale the image if necessary\n","        if min_dimension < required_dimension:\n","            scale_factor = required_dimension / min_dimension\n","            rescaled_image = cv2.resize(cropped_image, (0, 0), fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LINEAR)\n","        else:\n","            rescaled_image = cropped_image\n","            \n","\n","    return non_white_pixels_mask.ToPILImage(), rescaled_image.ToPILImage(), separated_rgb.ToPILImage()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:04.309914Z","iopub.status.busy":"2023-12-04T15:13:04.309339Z","iopub.status.idle":"2023-12-04T15:13:04.318384Z","shell.execute_reply":"2023-12-04T15:13:04.317518Z","shell.execute_reply.started":"2023-12-04T15:13:04.309888Z"},"trusted":true},"outputs":[],"source":["def create_random_box(image_width, image_height):\n","    # Determine the side length range for the square box (30% to 80% of the shorter image dimension)\n","    max_side_length = int(min(image_width, image_height) * 0.8)\n","    min_side_length = int(min(image_width, image_height) * 0.3)\n","\n","    # Randomly choose the side length of the square\n","    side_length = np.random.randint(min_side_length, max_side_length + 1)\n","\n","    # Ensure the square box fits within the image\n","    max_x = image_width - side_length\n","    max_y = image_height - side_length\n","\n","    x = np.random.randint(0, max_x + 1)\n","    y = np.random.randint(0, max_y + 1)\n","\n","    return x, y, side_length, side_length\n","\n","def modify_image(pil_image, box_x, box_y, box_width, box_height):\n","    # Convert PIL image to numpy array\n","    image_array = np.array(pil_image)\n","\n","    # Create a mask with the same size as the image, initialized to False\n","    mask = np.ones(image_array.shape[:2])\n","\n","    # Change all pixels in the box to 0 (black) in the image and set the mask to True in the box area\n","    image_array[box_y:box_y + box_height, box_x:box_x + box_width, :] = 0\n","    mask[box_y:box_y + box_height, box_x:box_x + box_width] = 0\n","\n","    # Extract the cropped image data from the original image\n","    cropped_image_data = image_array[box_y:box_y + box_height, box_x:box_x + box_width, :]\n","\n","    # Convert the numpy array back to a PIL image\n","    modified_image = Image.fromarray(image_array)\n","\n","    return cropped_image_data, mask, modified_image\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T15:13:04.319946Z","iopub.status.busy":"2023-12-04T15:13:04.319585Z","iopub.status.idle":"2023-12-04T15:13:04.332555Z","shell.execute_reply":"2023-12-04T15:13:04.331744Z","shell.execute_reply.started":"2023-12-04T15:13:04.319912Z"},"trusted":true},"outputs":[],"source":["mask_threshold = 0.1\n","def generate_latent_mask(original_latents, masked_latents):\n","    difference = torch.abs(original_latents - masked_latents)\n","\n","    # Create a binary mask using torch.where\n","    mask = torch.where(abs(difference > mask_threshold), torch.tensor(0), torch.tensor(1))\n","    #mask = torch.where(difference > mask_threshold, torch.tensor(0), torch.tensor(1))\n","\n","    #what is image=1, isn't 0\n","\n","    return mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T21:13:14.182482Z","iopub.status.busy":"2023-12-04T21:13:14.181709Z"},"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import io\n","import math\n","from PIL import Image\n","from tqdm import tqdm\n","\n","sampler.set_inference_timesteps(30)\n","l1_loss = nn.L1Loss()\n","lr = 0.0001\n","epochs = 2\n","clip_value = 1.0\n","# Optimizers\n","optimizer = optim.Adam(diffusion.parameters(), lr=lr)\n","criterion = nn.MSELoss()\n","# Training loop\n","for epoch in range(epochs):\n","    totalLoss = 0\n","    batch_idx = 0\n","    batch = []\n","    crops = []\n","    masked_images = []\n","    \n","    \n","    for item in train_dataset:\n","        \n","        # Extract the image from the 'target' attribute\n","        image_bytes = io.BytesIO(item['target']['bytes'])\n","        image = Image.open(image_bytes).convert('RGB')\n","        img_width, img_height = image.size\n","\n","\n","        \n","        \n","        x, y, width, height = create_random_box(img_width, img_height)\n","        \n","        cropped_image_data, mask, masked_image = modify_image(image, x,y,width, height)\n","        \n","        image = transforms(image)\n","        batch.append(image.to(device))\n","        crops.append(transforms(cropped_image_data).to(device))\n","        masked_images.append(transforms(masked_image).to(device))\n","\n","        if len(batch) == batch_size:\n","            batch_idx += 1\n","            images = torch.stack(batch)\n","            crops_stacked = torch.stack(crops)\n","            masked_images_stacked  = torch.stack(masked_images)\n","            \n","            with torch.no_grad():\n","                latents_without_noise = vae(images)\n","                encoded_crops = vae(crops_stacked)\n","                masked_image_latents = vae(masked_images_stacked)\n","                \n","                latent_mask = generate_latent_mask(latents_without_noise, masked_image_latents)\n","\n","            optimizer.zero_grad()\n","\n","            latents_shape = images.size()\n","            \n","            with torch.no_grad():\n","                latents = latent_mask * latents_without_noise + (1 - latent_mask) * sampler.add_noise(latents_without_noise, sampler.timesteps[0])\n","\n","            for i, timestep in enumerate(sampler.timesteps):\n","                # (1, 320)\n","                time_embedding = get_time_embedding(timestep).to(device)\n","\n","                # (Batch_Size, 4, Latents_Height, Latents_Width)\n","                model_input = latents\n","\n","                # (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 4, Latents_Height, Latents_Width)\n","                model_output = diffusion(model_input, time_embedding, encoded_crops)\n","\n","                # (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 4, Latents_Height, Latents_Width)\n","                latents = latent_mask * latents_without_noise + (1 - latent_mask) * sampler.step(timestep, latents, model_output)\n","\n","            masked_loss = criterion(latents, latents_without_noise)\n","            loss = masked_loss.mean()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(diffusion.parameters(), clip_value)\n","            optimizer.step()\n","            \n","            batch = []\n","            crops = []\n","            masked_images = []\n","            totalLoss+=(loss.item() / batch_size)\n","            \n","\n","            if batch_idx % 10 == 0:\n","                print('Train Epoch: {} [Batch {}] \\tLoss: {:.6f}'.format(\n","                    epoch, batch_idx, loss.item() / batch_size))\n","\n","    print('====> Epoch: {} Total loss: {:.4f}'.format(epoch, totalLoss))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save({\n","    'diffusion': diffusion.state_dict(),\n","}, '/kaggle/working/weights_4.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T21:12:41.921669Z","iopub.status.busy":"2023-12-04T21:12:41.921245Z","iopub.status.idle":"2023-12-04T21:12:41.949802Z","shell.execute_reply":"2023-12-04T21:12:41.948846Z","shell.execute_reply.started":"2023-12-04T21:12:41.921630Z"},"trusted":true},"outputs":[],"source":["checkpoint = torch.load(\"/kaggle/working/weights_2.pth\")\n","diffusion.load_state_dict(checkpoint['diffusion'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4104799,"sourceId":7117482,"sourceType":"datasetVersion"},{"datasetId":4107242,"sourceId":7120922,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
