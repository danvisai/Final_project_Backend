{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e0844f-ceb9-4301-8217-1d65aabdf69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\n",
      "--2023-11-28 20:50:00--  https://huggingface.co/dome272/wuerstchen/resolve/main/model_v2_stage_b.pt\n",
      "Resolving huggingface.co (huggingface.co)... 108.138.189.70, 108.138.189.96, 108.138.189.57, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.138.189.70|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/aba31f166a8dfb672fc81b63336b12e4667a10ddfb783a822b8fe20b356a899c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_v2_stage_b.pt%3B+filename%3D%22model_v2_stage_b.pt%22%3B&Expires=1701463801&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQ2MzgwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2FiYTMxZjE2NmE4ZGZiNjcyZmM4MWI2MzMzNmIxMmU0NjY3YTEwZGRmYjc4M2E4MjJiOGZlMjBiMzU2YTg5OWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=ZUkIz63sD%7ET7AF4BmGM3%7Ea4yI7psIPryJ8K%7E5jUo9AAOCSWTVkOKeRbcIJZ7mAE8mQJ4ZxjYl1DDMjWfzMxLnG6HduCF2jCwC9qZzlP3E%7EbncUg1QfwifYmcZfVNcWow1DkoWVpSE-Zqpa%7EKKKcbjRgUOBmdZMWOrktCNol5mYozZXGDbEvRvOY6ua4XdWvHnaUXJZRYI8kv3Pbmqfb7Ihjmcmn7J-HJQN3yMtfShhOjifWOl5zDbuDutTjx0J6fUkxQBv1qEziH5DJDEXew1y3G-JjJasVGCKRXRuIsIAQS2I0oWocOomzQM4uNjJKoLCtxeCmbtDKMT9B2gH2t8A__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-11-28 20:50:01--  https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/aba31f166a8dfb672fc81b63336b12e4667a10ddfb783a822b8fe20b356a899c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_v2_stage_b.pt%3B+filename%3D%22model_v2_stage_b.pt%22%3B&Expires=1701463801&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQ2MzgwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2FiYTMxZjE2NmE4ZGZiNjcyZmM4MWI2MzMzNmIxMmU0NjY3YTEwZGRmYjc4M2E4MjJiOGZlMjBiMzU2YTg5OWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=ZUkIz63sD%7ET7AF4BmGM3%7Ea4yI7psIPryJ8K%7E5jUo9AAOCSWTVkOKeRbcIJZ7mAE8mQJ4ZxjYl1DDMjWfzMxLnG6HduCF2jCwC9qZzlP3E%7EbncUg1QfwifYmcZfVNcWow1DkoWVpSE-Zqpa%7EKKKcbjRgUOBmdZMWOrktCNol5mYozZXGDbEvRvOY6ua4XdWvHnaUXJZRYI8kv3Pbmqfb7Ihjmcmn7J-HJQN3yMtfShhOjifWOl5zDbuDutTjx0J6fUkxQBv1qEziH5DJDEXew1y3G-JjJasVGCKRXRuIsIAQS2I0oWocOomzQM4uNjJKoLCtxeCmbtDKMT9B2gH2t8A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.66.218.53, 18.66.218.56, 18.66.218.17, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.66.218.53|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4303483515 (4.0G) [application/zip]\n",
      "Saving to: ‘models/model_v2_stage_b.pt’\n",
      "\n",
      "model_v2_stage_b.pt 100%[===================>]   4.01G   113MB/s    in 39s     \n",
      "\n",
      "2023-11-28 20:50:40 (105 MB/s) - ‘models/model_v2_stage_b.pt’ saved [4303483515/4303483515]\n",
      "\n",
      "--2023-11-28 20:50:40--  https://huggingface.co/dome272/wuerstchen/resolve/main/model_v2_stage_c_finetune_interpolation.pt\n",
      "Resolving huggingface.co (huggingface.co)... 108.138.189.74, 108.138.189.96, 108.138.189.57, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.138.189.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/d7178185bb6bf26a975e3afabe9baeacbc6424e97f636b97326d53043fd86ea5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_v2_stage_c_finetune_interpolation.pt%3B+filename%3D%22model_v2_stage_c_finetune_interpolation.pt%22%3B&Expires=1701463840&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQ2Mzg0MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2Q3MTc4MTg1YmI2YmYyNmE5NzVlM2FmYWJlOWJhZWFjYmM2NDI0ZTk3ZjYzNmI5NzMyNmQ1MzA0M2ZkODZlYTU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=FHDBbaIL24clwmKC5pubBengabUDuLRTG8hgUqn3IPSROJ-xiZbkDjkkbyKqLQaNvRajSUG0sm1wta%7EeL6aC95Bs7HYWOsQvEuVBM2I5mSZVrcypshOQ7ZRPUSftTS0ootxee06XBID-M3NvE74QyZyATWYBRFhuMPjWhErsYruQ6FfUFhwotanejyga0lDWltA9DmccU7CIxGdUCUiQX0vIgn%7EXlUVBaL5IU8gDBS4LOOoWCWnXxd9fOPFN6FXFW-kJILpR5lzJGXuZVpEYDC7D7ndFSoGjG8ONBwHx3lTgA5BoqLk1e0u3HvrvDVDdodTwTPUu1ltii8LCH7Okvg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-11-28 20:50:40--  https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/d7178185bb6bf26a975e3afabe9baeacbc6424e97f636b97326d53043fd86ea5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_v2_stage_c_finetune_interpolation.pt%3B+filename%3D%22model_v2_stage_c_finetune_interpolation.pt%22%3B&Expires=1701463840&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQ2Mzg0MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2Q3MTc4MTg1YmI2YmYyNmE5NzVlM2FmYWJlOWJhZWFjYmM2NDI0ZTk3ZjYzNmI5NzMyNmQ1MzA0M2ZkODZlYTU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=FHDBbaIL24clwmKC5pubBengabUDuLRTG8hgUqn3IPSROJ-xiZbkDjkkbyKqLQaNvRajSUG0sm1wta%7EeL6aC95Bs7HYWOsQvEuVBM2I5mSZVrcypshOQ7ZRPUSftTS0ootxee06XBID-M3NvE74QyZyATWYBRFhuMPjWhErsYruQ6FfUFhwotanejyga0lDWltA9DmccU7CIxGdUCUiQX0vIgn%7EXlUVBaL5IU8gDBS4LOOoWCWnXxd9fOPFN6FXFW-kJILpR5lzJGXuZVpEYDC7D7ndFSoGjG8ONBwHx3lTgA5BoqLk1e0u3HvrvDVDdodTwTPUu1ltii8LCH7Okvg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 3.161.119.90, 3.161.119.65, 3.161.119.5, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|3.161.119.90|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3974759010 (3.7G) [application/zip]\n",
      "Saving to: ‘models/model_v2_stage_c_finetune_interpolation.pt’\n",
      "\n",
      "model_v2_stage_c_fi 100%[===================>]   3.70G   126MB/s    in 34s     \n",
      "\n",
      "2023-11-28 20:51:15 (110 MB/s) - ‘models/model_v2_stage_c_finetune_interpolation.pt’ saved [3974759010/3974759010]\n",
      "\n",
      "--2023-11-28 20:51:15--  https://huggingface.co/dome272/wuerstchen/resolve/main/vqgan_f4_v1_500k.pt\n",
      "Resolving huggingface.co (huggingface.co)... 108.138.189.70, 108.138.189.96, 108.138.189.74, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.138.189.70|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/b4a81dd8733268a68d08844e33b917d4dbe27d5c5382dcbde522df3af16f343e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vqgan_f4_v1_500k.pt%3B+filename%3D%22vqgan_f4_v1_500k.pt%22%3B&Expires=1701463875&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQ2Mzg3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2I0YTgxZGQ4NzMzMjY4YTY4ZDA4ODQ0ZTMzYjkxN2Q0ZGJlMjdkNWM1MzgyZGNiZGU1MjJkZjNhZjE2ZjM0M2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=soYqaZC1P-fvqr%7E3ljfOrai4nUVdEYRYVb%7EGLU1d2okm4STvRxAFdfq-XOzpTd0Ffe9-s5OPwq9-yI9v-jipOMguaUVWactYkkf1DfH%7ECqOzqA0h%7EMWU057NFGiLMqSs-cX--hVdPMZJ%7E8R6%7E9IQRxWf7I56xCDqASsjUqJfiUII7-E2WOALXbUPYvg9yyU2yfQHXnpERbh2XzFkhXH8PHJl8i05TK91S4JVpmASQc8njEmxXrbbVdnOuZJrcQeMQt9nI92FAZlVva%7EPMSoP8rZD6dd5hoRm1aieISWJXPgW9ft2G4AHze8EeJxM0SNxHX8L0qlradKroR8s5z4bIQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-11-28 20:51:15--  https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/b4a81dd8733268a68d08844e33b917d4dbe27d5c5382dcbde522df3af16f343e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vqgan_f4_v1_500k.pt%3B+filename%3D%22vqgan_f4_v1_500k.pt%22%3B&Expires=1701463875&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTQ2Mzg3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2I0YTgxZGQ4NzMzMjY4YTY4ZDA4ODQ0ZTMzYjkxN2Q0ZGJlMjdkNWM1MzgyZGNiZGU1MjJkZjNhZjE2ZjM0M2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=soYqaZC1P-fvqr%7E3ljfOrai4nUVdEYRYVb%7EGLU1d2okm4STvRxAFdfq-XOzpTd0Ffe9-s5OPwq9-yI9v-jipOMguaUVWactYkkf1DfH%7ECqOzqA0h%7EMWU057NFGiLMqSs-cX--hVdPMZJ%7E8R6%7E9IQRxWf7I56xCDqASsjUqJfiUII7-E2WOALXbUPYvg9yyU2yfQHXnpERbh2XzFkhXH8PHJl8i05TK91S4JVpmASQc8njEmxXrbbVdnOuZJrcQeMQt9nI92FAZlVva%7EPMSoP8rZD6dd5hoRm1aieISWJXPgW9ft2G4AHze8EeJxM0SNxHX8L0qlradKroR8s5z4bIQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 3.161.119.65, 3.161.119.11, 3.161.119.5, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|3.161.119.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 296367311 (283M) [binary/octet-stream]\n",
      "Saving to: ‘models/vqgan_f4_v1_500k.pt.1’\n",
      "\n",
      "vqgan_f4_v1_500k.pt 100%[===================>] 282.64M  37.7MB/s    in 7.8s    \n",
      "\n",
      "2023-11-28 20:51:24 (36.2 MB/s) - ‘models/vqgan_f4_v1_500k.pt.1’ saved [296367311/296367311]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "!wget -P models https://huggingface.co/dome272/wuerstchen/resolve/main/model_v2_stage_b.pt\n",
    "!wget -P models https://huggingface.co/dome272/wuerstchen/resolve/main/model_v2_stage_c_finetune_interpolation.pt\n",
    "!wget -P models https://huggingface.co/dome272/wuerstchen/resolve/main/vqgan_f4_v1_500k.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f89f7ec-6252-4958-a5ac-48007ffcabe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/pabloppp/pytorch-tools\n",
      "  Cloning https://github.com/pabloppp/pytorch-tools to /tmp/pip-req-build-eqg_l34w\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/pabloppp/pytorch-tools /tmp/pip-req-build-eqg_l34w\n",
      "  Resolved https://github.com/pabloppp/pytorch-tools to commit 49d0c9234f5f2fed65c5bae3313767af10af2b3d\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m420.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from torchtools==0.3.8) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from torchtools==0.3.8) (0.16.0+cu118)\n",
      "Collecting ninja>=1.0 (from torchtools==0.3.8)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Downloading fsspec-2023.12.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.8) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.8) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.8) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.8) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->torchtools==0.3.8) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->torchtools==0.3.8) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->torchtools==0.3.8) (1.3.0)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.12.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: torchtools\n",
      "  Building wheel for torchtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchtools: filename=torchtools-0.3.8-py3-none-any.whl size=454024 sha256=91957cb8a859df04e5c17870416fc290e192d3213c676495ddc3a679754bae74\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-54o24ffn/wheels/8a/54/d6/da73a3a6b36c3f9309983d5e7665a21e9154ac5a9bff318fc9\n",
      "Successfully built torchtools\n",
      "Installing collected packages: ninja, tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers, torchtools\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2023.12.1 huggingface-hub-0.19.4 ninja-1.11.1.1 regex-2023.10.3 safetensors-0.4.1 tokenizers-0.15.0 torchtools-0.3.8 tqdm-4.66.1 transformers-4.35.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers git+https://github.com/pabloppp/pytorch-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc6c556-aae2-453c-ae23-3e435ed8f593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.46.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.2/156.2 kB\u001b[0m \u001b[31m514.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.46.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.46.0 kiwisolver-1.4.5 matplotlib-3.8.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448ecc6a-a210-4ee1-8379-6afafee2049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "#                        Code taken from Pablos implementation:                             #\n",
    "#    https://github.com/pabloppp/pytorch-tools/blob/master/torchtools/utils/diffusion.py    #\n",
    "#############################################################################################\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# Samplers --------------------------------------------------------------------\n",
    "class SimpleSampler():\n",
    "    def __init__(self, diffuzz):\n",
    "        self.current_step = -1\n",
    "        self.diffuzz = diffuzz\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.current_step += 1\n",
    "        return self.step(*args, **kwargs)\n",
    "\n",
    "    def init_x(self, shape):\n",
    "        return torch.randn(*shape, device=self.diffuzz.device)\n",
    "\n",
    "    def step(self, x, t, t_prev, noise):\n",
    "        raise NotImplementedError(\"You should override the 'apply' function.\")\n",
    "\n",
    "\n",
    "class DDPMSampler(SimpleSampler):\n",
    "    def step(self, x, t, t_prev, noise):\n",
    "        alpha_cumprod = self.diffuzz._alpha_cumprod(t).view(t.size(0), *[1 for _ in x.shape[1:]])\n",
    "        alpha_cumprod_prev = self.diffuzz._alpha_cumprod(t_prev).view(t_prev.size(0), *[1 for _ in x.shape[1:]])\n",
    "        alpha = (alpha_cumprod / alpha_cumprod_prev)\n",
    "\n",
    "        mu = (1.0 / alpha).sqrt() * (x - (1 - alpha) * noise / (1 - alpha_cumprod).sqrt())\n",
    "        std = ((1 - alpha) * (1. - alpha_cumprod_prev) / (1. - alpha_cumprod)).sqrt() * torch.randn_like(mu)\n",
    "        return mu + std * (t_prev != 0).float().view(t_prev.size(0), *[1 for _ in x.shape[1:]])\n",
    "\n",
    "\n",
    "class DDIMSampler(SimpleSampler):\n",
    "    def step(self, x, t, t_prev, noise):\n",
    "        alpha_cumprod = self.diffuzz._alpha_cumprod(t).view(t.size(0), *[1 for _ in x.shape[1:]])\n",
    "        alpha_cumprod_prev = self.diffuzz._alpha_cumprod(t_prev).view(t_prev.size(0), *[1 for _ in x.shape[1:]])\n",
    "\n",
    "        x0 = (x - (1 - alpha_cumprod).sqrt() * noise) / (alpha_cumprod).sqrt()\n",
    "        dp_xt = (1 - alpha_cumprod_prev).sqrt()\n",
    "        return (alpha_cumprod_prev).sqrt() * x0 + dp_xt * noise\n",
    "\n",
    "\n",
    "sampler_dict = {\n",
    "    'ddpm': DDPMSampler,\n",
    "    'ddim': DDIMSampler,\n",
    "}\n",
    "\n",
    "\n",
    "# Custom simplified foward/backward diffusion (cosine schedule)\n",
    "class Diffuzz():\n",
    "    def __init__(self, s=0.008, device=\"cpu\", cache_steps=None, scaler=1):\n",
    "        self.device = device\n",
    "        self.s = torch.tensor([s]).to(device)\n",
    "        self._init_alpha_cumprod = torch.cos(self.s / (1 + self.s) * torch.pi * 0.5) ** 2\n",
    "        self.scaler = scaler\n",
    "        self.cached_steps = None\n",
    "        if cache_steps is not None:\n",
    "            self.cached_steps = self._alpha_cumprod(torch.linspace(0, 1, cache_steps, device=device))\n",
    "\n",
    "    def _alpha_cumprod(self, t):\n",
    "        if self.cached_steps is None:\n",
    "            if self.scaler > 1:\n",
    "                t = 1 - (1 - t) ** self.scaler\n",
    "            elif self.scaler < 1:\n",
    "                t = t ** self.scaler\n",
    "            alpha_cumprod = torch.cos((t + self.s) / (1 + self.s) * torch.pi * 0.5) ** 2 / self._init_alpha_cumprod\n",
    "            return alpha_cumprod.clamp(0.0001, 0.9999)\n",
    "        else:\n",
    "            return self.cached_steps[t.mul(len(self.cached_steps) - 1).long()]\n",
    "\n",
    "    def diffuse(self, x, t, noise=None):  # t -> [0, 1]\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x)\n",
    "        alpha_cumprod = self._alpha_cumprod(t).view(t.size(0), *[1 for _ in x.shape[1:]])\n",
    "        return alpha_cumprod.sqrt() * x + (1 - alpha_cumprod).sqrt() * noise, noise\n",
    "\n",
    "    def undiffuse(self, x, t, t_prev, noise, sampler=None):\n",
    "        if sampler is None:\n",
    "            sampler = DDPMSampler(self)\n",
    "        return sampler(x, t, t_prev, noise)\n",
    "\n",
    "    def sample(self, model, model_inputs, shape, mask=None, t_start=1.0, t_end=0.0, timesteps=20, x_init=None, cfg=3.0,\n",
    "               unconditional_inputs=None, sampler='ddpm', half=False):\n",
    "        r_range = torch.linspace(t_start, t_end, timesteps + 1)[:, None].expand(-1, shape[\n",
    "            0] if x_init is None else x_init.size(0)).to(self.device)\n",
    "        if isinstance(sampler, str):\n",
    "            if sampler in sampler_dict:\n",
    "                sampler = sampler_dict[sampler](self)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"If sampler is a string it must be one of the supported samplers: {list(sampler_dict.keys())}\")\n",
    "        elif issubclass(sampler, SimpleSampler):\n",
    "            sampler = sampler(self)\n",
    "        else:\n",
    "            raise ValueError(\"Sampler should be either a string or a SimpleSampler object.\")\n",
    "        preds = []\n",
    "        x = sampler.init_x(shape) if x_init is None or mask is not None else x_init.clone()\n",
    "        if half:\n",
    "            r_range = r_range.half()\n",
    "            x = x.half()\n",
    "        if cfg is not None:\n",
    "            if unconditional_inputs is None:\n",
    "                unconditional_inputs = {k: torch.zeros_like(v) for k, v in model_inputs.items()}\n",
    "            model_inputs = {k: torch.cat([v, v_u]) for (k, v), (k_u, v_u) in\n",
    "                            zip(model_inputs.items(), unconditional_inputs.items())}\n",
    "        for i in range(0, timesteps):\n",
    "            if mask is not None and x_init is not None:\n",
    "                x_renoised, _ = self.diffuse(x_init, r_range[i])\n",
    "                x = x * mask + x_renoised * (1 - mask)\n",
    "\n",
    "            if cfg is not None:\n",
    "                pred_noise, pred_noise_unconditional = model(torch.cat([x] * 2), torch.cat([r_range[i]] * 2),\n",
    "                                                             **model_inputs).chunk(2)\n",
    "                pred_noise = torch.lerp(pred_noise_unconditional, pred_noise, cfg)\n",
    "            else:\n",
    "                pred_noise = model(x, r_range[i], **model_inputs)\n",
    "\n",
    "            x = self.undiffuse(x, r_range[i], r_range[i + 1], pred_noise, sampler=sampler)\n",
    "            preds.append(x)\n",
    "        return preds\n",
    "\n",
    "    def p2_weight(self, t, k=1.0, gamma=1.0):\n",
    "        alpha_cumprod = self._alpha_cumprod(t)\n",
    "        return (k + alpha_cumprod / (1 - alpha_cumprod)) ** -gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45063596-5b05-49ad-ac4c-f30d0c749d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class GlobalResponseNorm(nn.Module):  # from https://github.com/facebookresearch/ConvNeXt-V2/blob/3608f67cc1dae164790c5d0aead7bf2d73d9719b/models/utils.py#L105\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        return self.gamma * (x * Nx) + self.beta + x\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, c, c_cond, nhead, self_attn=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.norm = LayerNorm2d(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.attention = Attention2D(c, nhead, dropout)\n",
    "        self.kv_mapper = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(c_cond, c)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, kv):\n",
    "        kv = self.kv_mapper(kv)\n",
    "        x = x + self.attention(self.norm(x), kv, self_attn=self.self_attn)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, c, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm2d(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.channelwise = nn.Sequential(\n",
    "            nn.Linear(c, c * 4),\n",
    "            nn.GELU(),\n",
    "            GlobalResponseNorm(c * 4),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(c * 4, c)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.channelwise(self.norm(x).permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    def __init__(self, c, c_timestep):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Linear(c_timestep, c * 2)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        a, b = self.mapper(t)[:, :, None, None].chunk(2, dim=1)\n",
    "        return x * (1 + a) + b\n",
    "\n",
    "\n",
    "class Attention2D(nn.Module):\n",
    "    def __init__(self, c, nhead, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(c, nhead, dropout=dropout, bias=True, batch_first=True)\n",
    "\n",
    "    def forward(self, x, kv, self_attn=False):\n",
    "        orig_shape = x.shape\n",
    "        x = x.view(x.size(0), x.size(1), -1).permute(0, 2, 1)  # Bx4xHxW -> Bx(HxW)x4\n",
    "        if self_attn:\n",
    "            kv = torch.cat([x, kv], dim=1)\n",
    "        x = self.attn(x, kv, kv, need_weights=False)[0]\n",
    "        x = x.permute(0, 2, 1).view(*orig_shape)\n",
    "        return x\n",
    "\n",
    "class ResBlockStageB(nn.Module):\n",
    "    def __init__(self, c, c_skip=None, kernel_size=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(c, c, kernel_size=kernel_size, padding=kernel_size//2, groups=c)\n",
    "        self.norm = LayerNorm2d(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.channelwise = nn.Sequential(\n",
    "            nn.Linear(c+c_skip, c*4),\n",
    "            nn.GELU(),\n",
    "            GlobalResponseNorm(c*4),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(c*4, c)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_skip=None):\n",
    "        x_res = x\n",
    "        x = self.norm(self.depthwise(x))\n",
    "        if x_skip is not None:\n",
    "            x = torch.cat([x, x_skip], dim=1)\n",
    "        x = self.channelwise(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        return x + x_res\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c, c_skip=0, kernel_size=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(c + c_skip, c, kernel_size=kernel_size, padding=kernel_size // 2, groups=c)\n",
    "        self.norm = LayerNorm2d(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.channelwise = nn.Sequential(\n",
    "            nn.Linear(c, c * 4),\n",
    "            nn.GELU(),\n",
    "            GlobalResponseNorm(c * 4),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(c * 4, c)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_skip=None):\n",
    "        x_res = x\n",
    "        if x_skip is not None:\n",
    "            x = torch.cat([x, x_skip], dim=1)\n",
    "        x = self.norm(self.depthwise(x)).permute(0, 2, 3, 1)\n",
    "        x = self.channelwise(x).permute(0, 3, 1, 2)\n",
    "        return x + x_res\n",
    "\n",
    "\n",
    "class DiffNeXt(nn.Module):\n",
    "    def __init__(self, c_in=4, c_out=4, c_r=64, patch_size=2, c_cond=1024, c_hidden=[320, 640, 1280, 1280],\n",
    "                 nhead=[-1, 10, 20, 20], blocks=[4, 4, 14, 4], level_config=['CT', 'CTA', 'CTA', 'CTA'],\n",
    "                 inject_effnet=[False, True, True, True], effnet_embd=16, clip_embd=1024, kernel_size=3, dropout=0.1,\n",
    "                 self_attn=True):\n",
    "        super().__init__()\n",
    "        self.c_r = c_r\n",
    "        self.c_cond = c_cond\n",
    "        if not isinstance(dropout, list):\n",
    "            dropout = [dropout] * len(c_hidden)\n",
    "\n",
    "        # CONDITIONING\n",
    "        self.clip_mapper = nn.Linear(clip_embd, c_cond)\n",
    "        self.effnet_mappers = nn.ModuleList([\n",
    "            nn.Conv2d(effnet_embd, c_cond, kernel_size=1) if inject else None for inject in\n",
    "            inject_effnet + list(reversed(inject_effnet))\n",
    "        ])\n",
    "        self.seq_norm = nn.LayerNorm(c_cond, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.PixelUnshuffle(patch_size),\n",
    "            nn.Conv2d(c_in * (patch_size ** 2), c_hidden[0], kernel_size=1),\n",
    "            LayerNorm2d(c_hidden[0], elementwise_affine=False, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        def get_block(block_type, c_hidden, nhead, c_skip=0, dropout=0):\n",
    "            if block_type == 'C':\n",
    "                return ResBlockStageB(c_hidden, c_skip, kernel_size=kernel_size, dropout=dropout)\n",
    "            elif block_type == 'A':\n",
    "                return AttnBlock(c_hidden, c_cond, nhead, self_attn=self_attn, dropout=dropout)\n",
    "            elif block_type == 'F':\n",
    "                return FeedForwardBlock(c_hidden, dropout=dropout)\n",
    "            elif block_type == 'T':\n",
    "                return TimestepBlock(c_hidden, c_r)\n",
    "            else:\n",
    "                raise Exception(f'Block type {block_type} not supported')\n",
    "\n",
    "        # BLOCKS\n",
    "        # -- down blocks\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        for i in range(len(c_hidden)):\n",
    "            down_block = nn.ModuleList()\n",
    "            if i > 0:\n",
    "                down_block.append(nn.Sequential(\n",
    "                    LayerNorm2d(c_hidden[i - 1], elementwise_affine=False, eps=1e-6),\n",
    "                    nn.Conv2d(c_hidden[i - 1], c_hidden[i], kernel_size=2, stride=2),\n",
    "                ))\n",
    "            for _ in range(blocks[i]):\n",
    "                for block_type in level_config[i]:\n",
    "                    c_skip = c_cond if inject_effnet[i] else 0\n",
    "                    down_block.append(get_block(block_type, c_hidden[i], nhead[i], c_skip=c_skip, dropout=dropout[i]))\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # -- up blocks\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i in reversed(range(len(c_hidden))):\n",
    "            up_block = nn.ModuleList()\n",
    "            for j in range(blocks[i]):\n",
    "                for k, block_type in enumerate(level_config[i]):\n",
    "                    c_skip = c_hidden[i] if i < len(c_hidden) - 1 and j == k == 0 else 0\n",
    "                    c_skip += c_cond if inject_effnet[i] else 0\n",
    "                    up_block.append(get_block(block_type, c_hidden[i], nhead[i], c_skip=c_skip, dropout=dropout[i]))\n",
    "            if i > 0:\n",
    "                up_block.append(nn.Sequential(\n",
    "                    LayerNorm2d(c_hidden[i], elementwise_affine=False, eps=1e-6),\n",
    "                    nn.ConvTranspose2d(c_hidden[i], c_hidden[i - 1], kernel_size=2, stride=2),\n",
    "                ))\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # OUTPUT\n",
    "        self.clf = nn.Sequential(\n",
    "            LayerNorm2d(c_hidden[0], elementwise_affine=False, eps=1e-6),\n",
    "            nn.Conv2d(c_hidden[0], 2 * c_out * (patch_size ** 2), kernel_size=1),\n",
    "            nn.PixelShuffle(patch_size),\n",
    "        )\n",
    "\n",
    "        # --- WEIGHT INIT ---\n",
    "        self.apply(self._init_weights)  # General init\n",
    "        for mapper in self.effnet_mappers:\n",
    "            if mapper is not None:\n",
    "                nn.init.normal_(mapper.weight, std=0.02)  # conditionings\n",
    "        nn.init.normal_(self.clip_mapper.weight, std=0.02)  # conditionings\n",
    "        nn.init.xavier_uniform_(self.embedding[1].weight, 0.02)  # inputs\n",
    "        nn.init.constant_(self.clf[1].weight, 0)  # outputs\n",
    "\n",
    "        # blocks\n",
    "        for level_block in self.down_blocks + self.up_blocks:\n",
    "            for block in level_block:\n",
    "                if isinstance(block, ResBlockStageB) or isinstance(block, FeedForwardBlock):\n",
    "                    block.channelwise[-1].weight.data *= np.sqrt(1 / sum(blocks))\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    nn.init.constant_(block.mapper.weight, 0)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def gen_r_embedding(self, r, max_positions=10000):\n",
    "        r = r * max_positions\n",
    "        half_dim = self.c_r // 2\n",
    "        emb = math.log(max_positions) / (half_dim - 1)\n",
    "        emb = torch.arange(half_dim, device=r.device).float().mul(-emb).exp()\n",
    "        emb = r[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "        if self.c_r % 2 == 1:  # zero pad\n",
    "            emb = nn.functional.pad(emb, (0, 1), mode='constant')\n",
    "        return emb\n",
    "\n",
    "    def gen_c_embeddings(self, clip):\n",
    "        clip = self.clip_mapper(clip)\n",
    "        clip = self.seq_norm(clip)\n",
    "        return clip\n",
    "\n",
    "    def _down_encode(self, x, r_embed, effnet, clip):\n",
    "        level_outputs = []\n",
    "        for i, down_block in enumerate(self.down_blocks):\n",
    "            effnet_c = None\n",
    "            for block in down_block:\n",
    "                if isinstance(block, ResBlockStageB):\n",
    "                    if effnet_c is None and self.effnet_mappers[i] is not None:\n",
    "                        effnet_c = self.effnet_mappers[i](nn.functional.interpolate(\n",
    "                            effnet.float(), size=x.shape[-2:], mode='bicubic', antialias=True, align_corners=True\n",
    "                        ))\n",
    "                    skip = effnet_c if self.effnet_mappers[i] is not None else None\n",
    "                    x = block(x, skip)\n",
    "                elif isinstance(block, AttnBlock):\n",
    "                    x = block(x, clip)\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    x = block(x, r_embed)\n",
    "                else:\n",
    "                    x = block(x)\n",
    "            level_outputs.insert(0, x)\n",
    "        return level_outputs\n",
    "\n",
    "    def _up_decode(self, level_outputs, r_embed, effnet, clip):\n",
    "        x = level_outputs[0]\n",
    "        for i, up_block in enumerate(self.up_blocks):\n",
    "            effnet_c = None\n",
    "            for j, block in enumerate(up_block):\n",
    "                if isinstance(block, ResBlockStageB):\n",
    "                    if effnet_c is None and self.effnet_mappers[len(self.down_blocks) + i] is not None:\n",
    "                        effnet_c = self.effnet_mappers[len(self.down_blocks) + i](nn.functional.interpolate(\n",
    "                            effnet.float(), size=x.shape[-2:], mode='bicubic', antialias=True, align_corners=True\n",
    "                        ))\n",
    "                    skip = level_outputs[i] if j == 0 and i > 0 else None\n",
    "                    if effnet_c is not None:\n",
    "                        if skip is not None:\n",
    "                            skip = torch.cat([skip, effnet_c], dim=1)\n",
    "                        else:\n",
    "                            skip = effnet_c\n",
    "                    x = block(x, skip)\n",
    "                elif isinstance(block, AttnBlock):\n",
    "                    x = block(x, clip)\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    x = block(x, r_embed)\n",
    "                else:\n",
    "                    x = block(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, r, effnet, clip, x_cat=None, eps=1e-3, return_noise=True):\n",
    "        if x_cat is not None:\n",
    "            x = torch.cat([x, x_cat], dim=1)\n",
    "        # Process the conditioning embeddings\n",
    "        r_embed = self.gen_r_embedding(r)\n",
    "        clip = self.gen_c_embeddings(clip)\n",
    "\n",
    "        # Model Blocks\n",
    "        x_in = x\n",
    "        x = self.embedding(x)\n",
    "        level_outputs = self._down_encode(x, r_embed, effnet, clip)\n",
    "        x = self._up_decode(level_outputs, r_embed, effnet, clip)\n",
    "        a, b = self.clf(x).chunk(2, dim=1)\n",
    "        b = b.sigmoid() * (1 - eps * 2) + eps\n",
    "        if return_noise:\n",
    "            return (x_in - a) / b\n",
    "        else:\n",
    "            return a, b\n",
    "\n",
    "    def update_weights_ema(self, src_model, beta=0.999):\n",
    "        for self_params, src_params in zip(self.parameters(), src_model.parameters()):\n",
    "            self_params.data = self_params.data * beta + src_params.data * (1 - beta)\n",
    "\n",
    "\n",
    "class Paella(nn.Module):\n",
    "    def __init__(self, c_in=256, c_out=256, num_labels=8192, c_r=64, patch_size=2, c_cond=1024,\n",
    "                 c_hidden=[640, 1280, 1280], nhead=[-1, 16, 16], blocks=[4, 12, 4], level_config=['CT', 'CTA', 'CTA'],\n",
    "                 effnet_embd=16, byt5_embd=1536, kernel_size=3, dropout=0.1, self_attn=True):\n",
    "        super().__init__()\n",
    "        self.c_r = c_r\n",
    "        self.c_cond = c_cond\n",
    "        self.num_labels = num_labels\n",
    "        if not isinstance(dropout, list):\n",
    "            dropout = [dropout] * len(c_hidden)\n",
    "\n",
    "        # CONDITIONING\n",
    "        self.byt5_mapper = nn.Linear(byt5_embd, c_cond)\n",
    "        self.effnet_mapper = nn.Linear(effnet_embd, c_cond)\n",
    "        self.seq_norm = nn.LayerNorm(c_cond, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        self.in_mapper = nn.Sequential(\n",
    "            nn.Embedding(num_labels, c_in),\n",
    "            nn.LayerNorm(c_in, elementwise_affine=False, eps=1e-6)\n",
    "        )\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.PixelUnshuffle(patch_size),\n",
    "            nn.Conv2d(c_in * (patch_size ** 2), c_hidden[0], kernel_size=1),\n",
    "            LayerNorm2d(c_hidden[0], elementwise_affine=False, eps=1e-6)\n",
    "        )\n",
    "\n",
    "        def get_block(block_type, c_hidden, nhead, c_skip=0, dropout=0):\n",
    "            if block_type == 'C':\n",
    "                return ResBlock(c_hidden, c_skip, kernel_size=kernel_size, dropout=dropout)\n",
    "            elif block_type == 'A':\n",
    "                return AttnBlock(c_hidden, c_cond, nhead, self_attn=self_attn, dropout=dropout)\n",
    "            elif block_type == 'F':\n",
    "                return FeedForwardBlock(c_hidden, dropout=dropout)\n",
    "            elif block_type == 'T':\n",
    "                return TimestepBlock(c_hidden, c_r)\n",
    "            else:\n",
    "                raise Exception(f'Block type {block_type} not supported')\n",
    "\n",
    "        # BLOCKS\n",
    "        # -- down blocks\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        for i in range(len(c_hidden)):\n",
    "            down_block = nn.ModuleList()\n",
    "            if i > 0:\n",
    "                down_block.append(nn.Sequential(\n",
    "                    LayerNorm2d(c_hidden[i - 1], elementwise_affine=False, eps=1e-6),\n",
    "                    nn.Conv2d(c_hidden[i - 1], c_hidden[i], kernel_size=2, stride=2),\n",
    "                ))\n",
    "            for _ in range(blocks[i]):\n",
    "                for block_type in level_config[i]:\n",
    "                    down_block.append(get_block(block_type, c_hidden[i], nhead[i], dropout=dropout[i]))\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # -- up blocks\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i in reversed(range(len(c_hidden))):\n",
    "            up_block = nn.ModuleList()\n",
    "            for j in range(blocks[i]):\n",
    "                for k, block_type in enumerate(level_config[i]):\n",
    "                    up_block.append(get_block(block_type, c_hidden[i], nhead[i],\n",
    "                                              c_skip=c_hidden[i] if i < len(c_hidden) - 1 and j == k == 0 else 0,\n",
    "                                              dropout=dropout[i]))\n",
    "            if i > 0:\n",
    "                up_block.append(nn.Sequential(\n",
    "                    LayerNorm2d(c_hidden[i], elementwise_affine=False, eps=1e-6),\n",
    "                    nn.ConvTranspose2d(c_hidden[i], c_hidden[i - 1], kernel_size=2, stride=2),\n",
    "                ))\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # OUTPUT\n",
    "        self.clf = nn.Sequential(\n",
    "            LayerNorm2d(c_hidden[0], elementwise_affine=False, eps=1e-6),\n",
    "            nn.Conv2d(c_hidden[0], c_out * (patch_size ** 2), kernel_size=1),\n",
    "            nn.PixelShuffle(patch_size),\n",
    "        )\n",
    "        self.out_mapper = nn.Sequential(\n",
    "            LayerNorm2d(c_out, elementwise_affine=False, eps=1e-6),\n",
    "            nn.Conv2d(c_out, num_labels, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "        # --- WEIGHT INIT ---\n",
    "        self.apply(self._init_weights)  # General init\n",
    "\n",
    "        nn.init.normal_(self.byt5_mapper.weight, std=0.02)  # conditionings\n",
    "        nn.init.normal_(self.effnet_mapper.weight, std=0.02)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding[1].weight, 0.02)  # inputs\n",
    "        nn.init.constant_(self.clf[1].weight, 0)  # outputs\n",
    "        nn.init.normal_(self.in_mapper[0].weight, std=np.sqrt(1 / num_labels))  # out mapper\n",
    "        self.out_mapper[-1].weight.data = self.in_mapper[0].weight.data[:, :, None, None].clone()\n",
    "\n",
    "        # blocks\n",
    "        for level_block in self.down_blocks + self.up_blocks:\n",
    "            for block in level_block:\n",
    "                if isinstance(block, ResBlock) or isinstance(block, FeedForwardBlock):\n",
    "                    block.channelwise[-1].weight.data *= np.sqrt(1 / sum(blocks))\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    nn.init.constant_(block.mapper.weight, 0)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def gen_r_embedding(self, r, max_positions=10000):\n",
    "        r = r * max_positions\n",
    "        half_dim = self.c_r // 2\n",
    "        emb = math.log(max_positions) / (half_dim - 1)\n",
    "        emb = torch.arange(half_dim, device=r.device).float().mul(-emb).exp()\n",
    "        emb = r[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "        if self.c_r % 2 == 1:  # zero pad\n",
    "            emb = nn.functional.pad(emb, (0, 1), mode='constant')\n",
    "        return emb\n",
    "\n",
    "    def gen_c_embeddings(self, effnet, byt5):\n",
    "        effnet = effnet.permute(0, 2, 3, 1).view(effnet.size(0), -1, effnet.size(1))\n",
    "        seq = self.effnet_mapper(effnet)\n",
    "\n",
    "        if byt5 is not None:\n",
    "            byt5 = self.byt5_mapper(byt5)\n",
    "            seq = torch.cat([seq, byt5], dim=1)\n",
    "\n",
    "        seq = self.seq_norm(seq)\n",
    "        return seq\n",
    "\n",
    "    def _down_encode(self, x, r_embed, c_embed):\n",
    "        level_outputs = []\n",
    "        for down_block in self.down_blocks:\n",
    "            for block in down_block:\n",
    "                if isinstance(block, ResBlock):\n",
    "                    x = block(x)\n",
    "                elif isinstance(block, AttnBlock):\n",
    "                    x = block(x, c_embed)\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    x = block(x, r_embed)\n",
    "                else:\n",
    "                    x = block(x)\n",
    "            level_outputs.insert(0, x)\n",
    "        return level_outputs\n",
    "\n",
    "    def _up_decode(self, level_outputs, r_embed, c_embed):\n",
    "        x = level_outputs[0]\n",
    "        for i, up_block in enumerate(self.up_blocks):\n",
    "            for j, block in enumerate(up_block):\n",
    "                if isinstance(block, ResBlock):\n",
    "                    x = block(x, level_outputs[i] if j == 0 and i > 0 else None)\n",
    "                elif isinstance(block, AttnBlock):\n",
    "                    x = block(x, c_embed)\n",
    "                elif isinstance(block, TimestepBlock):\n",
    "                    x = block(x, r_embed)\n",
    "                else:\n",
    "                    x = block(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, r, effnet, byt5, x_cat=None):\n",
    "        if x_cat is not None:\n",
    "            x = torch.cat([x, x_cat], dim=1)\n",
    "        # Process the conditioning embeddings\n",
    "        r_embed = self.gen_r_embedding(r)\n",
    "        c_embed = self.gen_c_embeddings(effnet, byt5)\n",
    "\n",
    "        # Model Blocks\n",
    "        x = self.embedding(self.in_mapper(x).permute(0, 3, 1, 2))\n",
    "        level_outputs = self._down_encode(x, r_embed, c_embed)\n",
    "        x = self._up_decode(level_outputs, r_embed, c_embed)\n",
    "        x = self.out_mapper(self.clf(x))\n",
    "        return x\n",
    "\n",
    "    def add_noise(self, x, t, mask=None, random_x=None):\n",
    "        if mask is None:\n",
    "            mask = (torch.rand_like(x.float()) <= t[:, None, None]).long()\n",
    "        if random_x is None:\n",
    "            random_x = torch.randint_like(x, 0, self.num_labels)\n",
    "        x = x * (1 - mask) + random_x * mask\n",
    "        return x, mask\n",
    "\n",
    "    def get_loss_weight(self, t, mask, min_val=0.3):\n",
    "        return 1 - (1 - mask) * ((1 - t) * (1 - min_val))[:, None, None]\n",
    "\n",
    "    def update_weights_ema(self, src_model, beta=0.999):\n",
    "        for self_params, src_params in zip(self.parameters(), src_model.parameters()):\n",
    "            self_params.data = self_params.data * beta + src_params.data * (1 - beta)\n",
    "\n",
    "\n",
    "def sample(model, model_inputs, latent_shape, unconditional_inputs=None, init_x=None, steps=12, renoise_steps=None, temperature = (0.7, 0.3), cfg=(8.0, 8.0), mode = 'multinomial', t_start=1.0, t_end=0.0, sampling_conditional_steps=None, sampling_quant_steps=None, attn_weights=None): # 'quant', 'multinomial', 'argmax'\n",
    "    device = unconditional_inputs[\"byt5\"].device\n",
    "    if sampling_conditional_steps is None:\n",
    "        sampling_conditional_steps = steps\n",
    "    if sampling_quant_steps is None:\n",
    "        sampling_quant_steps = steps\n",
    "    if renoise_steps is None:\n",
    "        renoise_steps = steps-1\n",
    "    if unconditional_inputs is None:\n",
    "        unconditional_inputs = {k: torch.zeros_like(v) for k, v in model_inputs.items()}\n",
    "    intermediate_images = []\n",
    "    # with torch.inference_mode():\n",
    "    init_noise = torch.randint(0, model.num_labels, size=latent_shape, device=device)\n",
    "    if init_x != None:\n",
    "        sampled = init_x\n",
    "    else:\n",
    "        sampled = init_noise.clone()\n",
    "    t_list = torch.linspace(t_start, t_end, steps+1)\n",
    "    temperatures = torch.linspace(temperature[0], temperature[1], steps)\n",
    "    cfgs = torch.linspace(cfg[0], cfg[1], steps)\n",
    "    if cfg is not None:\n",
    "        model_inputs = {k:torch.cat([v, v_u]) for (k, v), (k_u, v_u) in zip(model_inputs.items(), unconditional_inputs.items())}\n",
    "    for i, tv in enumerate(t_list[:steps]):\n",
    "        if i >= sampling_quant_steps:\n",
    "            mode = \"quant\"\n",
    "        t = torch.ones(latent_shape[0], device=device) * tv\n",
    "\n",
    "        if cfg is not None and i < sampling_conditional_steps:\n",
    "            logits, uncond_logits = model(torch.cat([sampled]*2), torch.cat([t]*2), **model_inputs).chunk(2)\n",
    "            logits = logits * cfgs[i] + uncond_logits * (1-cfgs[i])\n",
    "        else:\n",
    "            logits = model(sampled, t, **model_inputs)\n",
    "\n",
    "        scores = logits.div(temperatures[i]).softmax(dim=1)\n",
    "\n",
    "        if mode == 'argmax':\n",
    "            sampled = logits.argmax(dim=1)\n",
    "        elif mode == 'multinomial':\n",
    "            sampled = scores.permute(0, 2, 3, 1).reshape(-1, logits.size(1))\n",
    "            sampled = torch.multinomial(sampled, 1)[:, 0].view(logits.size(0), *logits.shape[2:])\n",
    "        elif mode == 'quant':\n",
    "            sampled = scores.permute(0, 2, 3, 1) @ vqmodel.vquantizer.codebook.weight.data\n",
    "            sampled = vqmodel.vquantizer.forward(sampled, dim=-1)[-1]\n",
    "        else:\n",
    "            raise Exception(f\"Mode '{mode}' not supported, use: 'quant', 'multinomial' or 'argmax'\")\n",
    "\n",
    "        intermediate_images.append(sampled)\n",
    "\n",
    "        if i < renoise_steps:\n",
    "            t_next = torch.ones(latent_shape[0], device=device) * t_list[i+1]\n",
    "            sampled = model.add_noise(sampled, t_next, random_x=init_noise)[0]\n",
    "            intermediate_images.append(sampled)\n",
    "    return sampled, intermediate_images\n",
    "\n",
    "\n",
    "class EfficientNetEncoder(nn.Module):\n",
    "    def __init__(self, c_latent=16, effnet=\"efficientnet_v2_s\"):\n",
    "        super().__init__()\n",
    "        if effnet == \"efficientnet_v2_s\":\n",
    "            self.backbone = torchvision.models.efficientnet_v2_s(weights='DEFAULT').features.eval()\n",
    "        else:\n",
    "            print(\"Using EffNet L.\")\n",
    "            self.backbone = torchvision.models.efficientnet_v2_l(weights='DEFAULT').features.eval()\n",
    "        self.mapper = nn.Sequential(\n",
    "            nn.Conv2d(1280, c_latent, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(c_latent),  # then normalize them to have mean 0 and std 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mapper(self.backbone(x)).add(1.).div(42.)\n",
    "\n",
    "\n",
    "class Wrapper(nn.Module):\n",
    "    def __init__(self, effnet, generator, device=None):\n",
    "        super().__init__()\n",
    "        self.effnet = effnet\n",
    "        self.generator = generator\n",
    "        self.diffuzz = Diffuzz(device=device)\n",
    "\n",
    "    def forward(self, x, r, effnet_x, byt5, x_cat=None):\n",
    "        effnet = self.effnet(effnet_x)\n",
    "        if np.random.rand() < 0.05:\n",
    "            effnet = 0 * effnet\n",
    "        elif np.random.rand() < 0.3:\n",
    "            effnet = self.noise_effnet_embeds(effnet)\n",
    "        return self.generator(x, r, effnet, byt5, x_cat)\n",
    "\n",
    "    def noise_effnet_embeds(self, effnet):\n",
    "        noise_t = torch.rand(effnet.size(0), device=effnet.device) * 0.5\n",
    "        effnet = self.diffuzz.diffuse(effnet, noise_t)[0]\n",
    "        return effnet\n",
    "\n",
    "    def update_weights_ema(self, src_model, beta=0.999):\n",
    "        for self_params, src_params in zip(self.parameters(), src_model.parameters()):\n",
    "            self_params.data = self_params.data * beta + src_params.data * (1 - beta)\n",
    "\n",
    "\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self, c_in=16, c=1280, c_cond=1024, c_r=64, depth=16, nhead=16, latent_size=(12, 12), dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.c_r = c_r\n",
    "        self.projection = nn.Conv2d(c_in, c, kernel_size=1)\n",
    "        self.cond_mapper = nn.Sequential(\n",
    "            nn.Linear(c_cond, c),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(c, c),\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(depth):\n",
    "            self.blocks.append(ResBlock(c, dropout=dropout))\n",
    "            self.blocks.append(TimestepBlock(c, c_r))\n",
    "            self.blocks.append(AttnBlock(c, c, nhead, self_attn=True, dropout=dropout))\n",
    "        self.out = nn.Sequential(\n",
    "            LayerNorm2d(c, elementwise_affine=False, eps=1e-6),\n",
    "            nn.Conv2d(c, c_in * 2, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)  # General init\n",
    "        nn.init.normal_(self.projection.weight, std=0.02)  # inputs\n",
    "        nn.init.normal_(self.cond_mapper[0].weight, std=0.02)  # conditionings\n",
    "        nn.init.normal_(self.cond_mapper[-1].weight, std=0.02)  # conditionings\n",
    "        nn.init.constant_(self.out[1].weight, 0)  # outputs\n",
    "\n",
    "        # blocks\n",
    "        for block in self.blocks:\n",
    "            if isinstance(block, ResBlock):\n",
    "                block.channelwise[-1].weight.data *= np.sqrt(1 / depth)\n",
    "            elif isinstance(block, TimestepBlock):\n",
    "                nn.init.constant_(block.mapper.weight, 0)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def gen_r_embedding(self, r, max_positions=10000):\n",
    "        r = r * max_positions\n",
    "        half_dim = self.c_r // 2\n",
    "        emb = math.log(max_positions) / (half_dim - 1)\n",
    "        emb = torch.arange(half_dim, device=r.device).float().mul(-emb).exp()\n",
    "        emb = r[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "        if self.c_r % 2 == 1:  # zero pad\n",
    "            emb = nn.functional.pad(emb, (0, 1), mode='constant')\n",
    "        return emb\n",
    "\n",
    "    def forward(self, x, r, c):\n",
    "        x_in = x\n",
    "        x = self.projection(x)\n",
    "        c_embed = self.cond_mapper(c)\n",
    "        r_embed = self.gen_r_embedding(r)\n",
    "        for block in self.blocks:\n",
    "            if isinstance(block, AttnBlock):\n",
    "                x = block(x, c_embed)\n",
    "            elif isinstance(block, TimestepBlock):\n",
    "                x = block(x, r_embed)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        a, b = self.out(x).chunk(2, dim=1)\n",
    "        # denoised = a / (1-(1-b).pow(2)).sqrt()\n",
    "        return (x_in - a) / ((1 - b).abs() + 1e-5)\n",
    "\n",
    "    def update_weights_ema(self, src_model, beta=0.999):\n",
    "        for self_params, src_params in zip(self.parameters(), src_model.parameters()):\n",
    "            self_params.data = self_params.data * beta + src_params.data * (1 - beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e647dce-5cd0-48cf-91af-e809debfc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torchtools.nn import VectorQuantize\n",
    "\n",
    "class VQResBlock(nn.Module):\n",
    "    def __init__(self, c, c_hidden):\n",
    "        super().__init__()\n",
    "        # depthwise/attention\n",
    "        self.norm1 = nn.LayerNorm(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.ReplicationPad2d(1),\n",
    "            nn.Conv2d(c, c, kernel_size=3, groups=c)\n",
    "        )\n",
    "\n",
    "        # channelwise\n",
    "        self.norm2 = nn.LayerNorm(c, elementwise_affine=False, eps=1e-6)\n",
    "        self.channelwise = nn.Sequential(\n",
    "            nn.Linear(c, c_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(c_hidden, c),\n",
    "        )\n",
    "        \n",
    "        self.gammas = nn.Parameter(torch.zeros(6),  requires_grad=True)\n",
    "        \n",
    "        # Init weights\n",
    "        def _basic_init(module):\n",
    "            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "        self.apply(_basic_init)\n",
    "\n",
    "\n",
    "    def _norm(self, x, norm):\n",
    "        return norm(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mods = self.gammas\n",
    "            \n",
    "        x_temp = self._norm(x, self.norm1) * (1 + mods[0]) + mods[1]\n",
    "        x = x + self.depthwise(x_temp) * mods[2]\n",
    "\n",
    "        x_temp = self._norm(x, self.norm2) * (1 + mods[3]) + mods[4]\n",
    "        x = x + self.channelwise(x_temp.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) * mods[5]\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VQModel(nn.Module):\n",
    "    def __init__(self, levels=2, bottleneck_blocks=12, c_hidden=384, c_latent=4, codebook_size=8192, scale_factor=0.3764): # 1.0\n",
    "        super().__init__()\n",
    "        self.c_latent = c_latent\n",
    "        self.scale_factor = scale_factor\n",
    "        c_levels = [c_hidden//(2**i) for i in reversed(range(levels))]\n",
    "        \n",
    "        # Encoder blocks\n",
    "        self.in_block = nn.Sequential(\n",
    "            nn.PixelUnshuffle(2),\n",
    "            nn.Conv2d(3*4, c_levels[0], kernel_size=1)\n",
    "        )\n",
    "        down_blocks = []\n",
    "        for i in range(levels):\n",
    "            if i > 0:\n",
    "                down_blocks.append(nn.Conv2d(c_levels[i-1], c_levels[i], kernel_size=4, stride=2, padding=1))\n",
    "            block = VQResBlock(c_levels[i], c_levels[i]*4)\n",
    "            down_blocks.append(block)\n",
    "        down_blocks.append(nn.Sequential(\n",
    "            nn.Conv2d(c_levels[-1], c_latent, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(c_latent), # then normalize them to have mean 0 and std 1\n",
    "        ))\n",
    "        self.down_blocks = nn.Sequential(*down_blocks)\n",
    "        self.down_blocks[0]\n",
    "        \n",
    "        self.codebook_size = codebook_size\n",
    "        self.vquantizer = VectorQuantize(c_latent, k=codebook_size)        \n",
    "\n",
    "        # Decoder blocks\n",
    "        up_blocks = [nn.Sequential(\n",
    "            nn.Conv2d(c_latent, c_levels[-1], kernel_size=1)\n",
    "        )]\n",
    "        for i in range(levels):\n",
    "            for j in range(bottleneck_blocks if i == 0 else 1):\n",
    "                block = VQResBlock(c_levels[levels-1-i], c_levels[levels-1-i]*4)\n",
    "                up_blocks.append(block)\n",
    "            if i < levels-1:\n",
    "                up_blocks.append(nn.ConvTranspose2d(c_levels[levels-1-i], c_levels[levels-2-i], kernel_size=4, stride=2, padding=1))\n",
    "        self.up_blocks = nn.Sequential(*up_blocks)\n",
    "        self.out_block = nn.Sequential(\n",
    "            nn.Conv2d(c_levels[0], 3*4, kernel_size=1),\n",
    "            nn.PixelShuffle(2),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.in_block(x)\n",
    "        x = self.down_blocks(x)\n",
    "        qe, (vq_loss, commit_loss), indices = self.vquantizer.forward(x, dim=1)\n",
    "        return qe / self.scale_factor, x / self.scale_factor, indices, vq_loss + commit_loss * 0.25\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = x * self.scale_factor\n",
    "        x = self.up_blocks(x)\n",
    "        x = self.out_block(x)\n",
    "        return x\n",
    "\n",
    "    def decode_indices(self, x):\n",
    "        x = self.vquantizer.idx2vq(x, dim=1)\n",
    "        x = self.up_blocks(x)\n",
    "        x = self.out_block(x)\n",
    "        return x\n",
    "\n",
    "    def encode_img(self, x):\n",
    "        x = self.in_block(x)\n",
    "        x = self.down_blocks(x)\n",
    "        qe, (vq_loss, commit_loss), indices = self.vquantizer.forward(x, dim=1)\n",
    "        return qe\n",
    "        \n",
    "    def forward(self, x, quantize=False):\n",
    "        qe, x, _, vq_loss = self.encode(x, quantize)\n",
    "        x = self.decode(qe)\n",
    "        return x, vq_loss\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, c_in=3, c_cond=0, c_hidden=512, depth=6):\n",
    "        super().__init__()\n",
    "        d = max(depth - 3, 3)\n",
    "        layers = [\n",
    "            nn.utils.spectral_norm(nn.Conv2d(c_in, c_hidden // (2 ** d), kernel_size=3, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        ]\n",
    "        for i in range(depth - 1):\n",
    "            c_in = c_hidden // (2 ** max((d - i), 0))\n",
    "            c_out = c_hidden // (2 ** max((d - 1 - i), 0))\n",
    "            layers.append(nn.utils.spectral_norm(nn.Conv2d(c_in, c_out, kernel_size=3, stride=2, padding=1)))\n",
    "            layers.append(nn.InstanceNorm2d(c_out))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.shuffle = nn.Conv2d((c_hidden + c_cond) if c_cond > 0 else c_hidden, 1, kernel_size=1)\n",
    "        self.logits = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, cond=None):\n",
    "        x = self.encoder(x)\n",
    "        if cond is not None:\n",
    "            cond = cond.view(cond.size(0), cond.size(1), 1, 1, ).expand(-1, -1, x.size(-2), x.size(-1))\n",
    "            x = torch.cat([x, cond], dim=1)\n",
    "        x = self.shuffle(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373230e1-945f-4b09-bfe8-558a105f5720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351b2fb47562460296b13d878a902581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503d295ef110402c8c6eb5751938519b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/120k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b56cf5486545cf973424a5475315d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868446d1c9644350bbcb1ca67df5f8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, T5EncoderModel, CLIPTextModel\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "checkpoint_stage_a = \"/workspace/models/vqgan_f4_v1_500k.pt\"\n",
    "checkpoint_stage_b = \"/workspace/models/model_v2_stage_b.pt\"\n",
    "checkpoint_stage_c = \"/workspace/models/model_v2_stage_c_finetune_interpolation.pt\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "effnet_preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(768, interpolation=torchvision.transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "    torchvision.transforms.CenterCrop(768),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "def encode(vqmodel, img_seq):\n",
    "    return vqmodel.encode_img(img_seq)\n",
    "\n",
    "def decode(img_seq):\n",
    "    return vqmodel.decode(img_seq)\n",
    "\n",
    "def embed_clip(clip_tokenizer, clip_model, caption, negative_caption=\"\", batch_size=4, device=\"cuda\"):\n",
    "    clip_tokens = clip_tokenizer([caption] * batch_size, truncation=True, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\").to(device)\n",
    "    clip_text_embeddings = clip_model(**clip_tokens).last_hidden_state\n",
    "\n",
    "    clip_tokens_uncond = clip_tokenizer([negative_caption] * batch_size, truncation=True, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\").to(device)\n",
    "    clip_text_embeddings_uncond = clip_model(**clip_tokens_uncond).last_hidden_state\n",
    "    return clip_text_embeddings, clip_text_embeddings_uncond\n",
    "\n",
    "vqmodel = VQModel().to(device)\n",
    "vqmodel.load_state_dict(torch.load(checkpoint_stage_a, map_location=device)[\"state_dict\"])\n",
    "vqmodel.eval().requires_grad_(False)\n",
    "\n",
    "clip_model = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\").to(device).eval().requires_grad_(False)\n",
    "clip_tokenizer = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\")\n",
    "\n",
    "clip_model_b = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\").eval().requires_grad_(False).to(device)\n",
    "clip_tokenizer_b = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "\n",
    "diffuzz = Diffuzz(device=device)\n",
    "\n",
    "pretrained_checkpoint = torch.load(checkpoint_stage_b, map_location=device)\n",
    "\n",
    "effnet = EfficientNetEncoder().to(device)\n",
    "effnet.load_state_dict(pretrained_checkpoint['effnet_state_dict'])\n",
    "effnet.eval().requires_grad_(False)\n",
    "\n",
    "# - LDM Model as generator -\n",
    "generator = DiffNeXt()\n",
    "generator.load_state_dict(pretrained_checkpoint['state_dict'])\n",
    "generator.eval().requires_grad_(False).to(device)\n",
    "\n",
    "del pretrained_checkpoint\n",
    "\n",
    "checkpoint = torch.load(checkpoint_stage_c, map_location=device)\n",
    "model = Prior(c_in=16, c=1536, c_cond=1280, c_r=64, depth=32, nhead=24).to(device)\n",
    "model.load_state_dict(checkpoint['ema_state_dict'])\n",
    "model.eval().requires_grad_(False)\n",
    "del checkpoint\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a30529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_clip_embeddings(clip_tokenizer, clip_model, sentences, batch_size, device):\n",
    "    all_embeddings = []\n",
    "    for sentence in sentences:\n",
    "        embeddings, _ = embed_clip(clip_tokenizer, clip_model, sentence, batch_size=batch_size, device=device)\n",
    "        all_embeddings.append(embeddings.mean(dim=0))\n",
    "    return torch.stack(all_embeddings).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20172725-e2eb-425b-bcad-13173d9656ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "batch_size = 1\n",
    "prior_inference_steps = {2/3: 20, 0.0: 10}\n",
    "prior_cfg = 4\n",
    "prior_sampler = \"ddpm\"\n",
    "\n",
    "generator_steps = 12\n",
    "generator_cfg = None\n",
    "generator_sampler = \"ddpm\"\n",
    "\n",
    "height = 1024\n",
    "width = 1024\n",
    "\n",
    "def generateImage(caption, negative_caption):\n",
    "    effnet_features_shape = (batch_size, 16, 12, 12)\n",
    "    \n",
    "    clip_text_embeddings, clip_text_embeddings_uncond = embed_clip(clip_tokenizer, clip_model, caption, negative_caption, batch_size, device)\n",
    "    \n",
    "    latent_height = 128 * (height // 128) // (1024 // 24)\n",
    "    latent_width = 128 * (width // 128) // (1024 // 24)\n",
    "    prior_features_shape = (batch_size, 16, latent_height, latent_width)\n",
    "    effnet_embeddings_uncond = torch.zeros(effnet_features_shape).to(device)\n",
    "    generator_latent_shape = (batch_size, 4, int(latent_height * (256 / 24)), int(latent_width * (256 / 24)))\n",
    "    # torch.manual_seed(42)\n",
    "    \n",
    "    effnet_preprocess = transforms.Compose([\n",
    "        transforms.Resize(384, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "        transforms.CenterCrop(384),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.485, 0.456, 0.406), \n",
    "            std=(0.229, 0.224, 0.225)\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    transformedImage = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(1024),\n",
    "        torchvision.transforms.CenterCrop(1024),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16), torch.no_grad():\n",
    "        s = time.time()\n",
    "        t_start = 1.0\n",
    "        sampled = None\n",
    "        for t_end, steps in prior_inference_steps.items():\n",
    "            sampled = diffuzz.sample(model, {'c': clip_text_embeddings}, x_init=sampled, unconditional_inputs={\"c\": clip_text_embeddings_uncond}, shape=prior_features_shape,\n",
    "                                timesteps=steps, cfg=prior_cfg, sampler=prior_sampler,\n",
    "                                t_start=t_start, t_end=t_end)[-1]\n",
    "            t_start = t_end\n",
    "        sampled = sampled.mul(42).sub(1)\n",
    "    \n",
    "        print(f\"Prior Sampling: {time.time() - s}\")\n",
    "    \n",
    "        clip_text_embeddings, clip_text_embeddings_uncond = embed_clip(clip_tokenizer_b, clip_model_b, caption, negative_caption, batch_size, device)\n",
    "    \n",
    "        s = time.time()\n",
    "        sampled_images_original = diffuzz.sample(generator, {'effnet': sampled, 'clip': clip_text_embeddings},\n",
    "                                generator_latent_shape, t_start=1.0, t_end=0.00,\n",
    "                                timesteps=generator_steps, cfg=generator_cfg, sampler=generator_sampler,\n",
    "                                unconditional_inputs = {\n",
    "                                'effnet': effnet_embeddings_uncond, 'clip': clip_text_embeddings_uncond,\n",
    "                            })[-1]\n",
    "        print(f\"Generator Sampling: {time.time() - s}\")\n",
    "    \n",
    "    s = time.time()\n",
    "    sampled = decode(sampled_images_original)\n",
    "    print(f\"Decoder Generation: {time.time() - s}\")\n",
    "    print(f\"Prior => CFG: {prior_cfg}, Steps: {sum(prior_inference_steps.values())}, Sampler: {prior_sampler}\")\n",
    "    print(f\"Generator => CFG: {generator_cfg}, Steps: {generator_steps}, Sampler: {generator_sampler}\")\n",
    "    print(f\"Images Shape: {sampled.shape}\")\n",
    "    print(caption)\n",
    "    duration_seconds = time.time() - s\n",
    "    print(duration_seconds)\n",
    "    \n",
    "    minutes = int(duration_seconds // 60)\n",
    "    seconds = int(duration_seconds % 60)\n",
    "    formatted_time = f\"Time taken: {minutes} minutes and {seconds} seconds\"\n",
    "\n",
    "    return sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb92a21-0e92-4632-ab3b-5bd1bd0b8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833b4cc-96dd-4a60-b313-79de52743ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c0e69-a6ae-4a40-b02f-bb9cbd36bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install werkzeug===2.2.2\n",
    "!pip install flask==2.2.1\n",
    "!pip install flask_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793d670-e63a-44c6-a108-d29128c5aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, send_file  # import main Flask class and request object\n",
    "from flask_cors import CORS\n",
    "import random\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import io\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b704c51-8bd6-4bf0-842d-6fb9b89fa05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "app1 = Flask(__name__)  # create the Flask app\n",
    "CORS(app1)\n",
    "@app1.route('/get_image', methods=['POST'])\n",
    "def get_edit():\n",
    "    text_input = request.form['prompt']\n",
    "    neg_str = request.values['negative']\n",
    "    positivePrompt = \" cold color palette, muted colors, detailed, 8k\"\n",
    "    negativePrompt = \" blurry, low quality\"\n",
    "    if text_input is not None:\n",
    "        positivePrompt = text_input + positivePrompt\n",
    "    if neg_str is not None:\n",
    "        negativePrompt = neg_str + negativePrompt\n",
    "\n",
    "    data = generateImage(positivePrompt, negativePrompt)[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    data = transforms.ToPILImage()(data)\n",
    "    img_io = BytesIO()\n",
    "    data.save(img_io, format='JPEG')\n",
    "    img_io.seek(0)\n",
    "    return send_file(img_io, mimetype='image/jpg')\n",
    "\n",
    "app1.run(host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0ca83-611e-4172-96b9-6cd18942c6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
