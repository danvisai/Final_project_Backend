{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","import numpy as np\n","import torchvision\n","from torch import nn\n","import torch\n","from torch.utils.data import DataLoader, IterableDataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T16:53:48.466437Z","iopub.status.busy":"2023-12-04T16:53:48.465998Z","iopub.status.idle":"2023-12-04T16:53:50.621871Z","shell.execute_reply":"2023-12-04T16:53:50.620848Z","shell.execute_reply.started":"2023-12-04T16:53:48.466410Z"},"trusted":true},"outputs":[],"source":["train_dataset = load_dataset(\"mingyy/chinese_landscape_paintings\", split='train', streaming=True)\n","class StreamingDataLoader(IterableDataset):\n","    def __init__(self, dataset, batch_size):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","\n","    def __iter__(self):\n","        batch = []\n","        for item in self.dataset:\n","            batch.append(item)\n","            if len(batch) == self.batch_size:\n","                yield batch\n","                batch = []\n","        if batch:\n","            yield batch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T16:53:50.623704Z","iopub.status.busy":"2023-12-04T16:53:50.623404Z","iopub.status.idle":"2023-12-04T16:53:50.628026Z","shell.execute_reply":"2023-12-04T16:53:50.627031Z","shell.execute_reply.started":"2023-12-04T16:53:50.623662Z"},"trusted":true},"outputs":[],"source":["def streaming_map(dataset, transform_fn):\n","    for sample in dataset:\n","        yield transform_fn(sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T16:53:50.629417Z","iopub.status.busy":"2023-12-04T16:53:50.629114Z","iopub.status.idle":"2023-12-04T16:53:50.637643Z","shell.execute_reply":"2023-12-04T16:53:50.636915Z","shell.execute_reply.started":"2023-12-04T16:53:50.629377Z"},"trusted":true},"outputs":[],"source":["\n","transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Resize(512),\n","    #torchvision.transforms.RandomCrop(512),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T16:53:50.640132Z","iopub.status.busy":"2023-12-04T16:53:50.639829Z","iopub.status.idle":"2023-12-04T16:53:50.646752Z","shell.execute_reply":"2023-12-04T16:53:50.645908Z","shell.execute_reply.started":"2023-12-04T16:53:50.640096Z"},"trusted":true},"outputs":[],"source":["batch_size=64\n","\n","\n","transformed_dataset = streaming_map(train_dataset, transforms)\n","dataloader = DataLoader(StreamingDataLoader(transformed_dataset, batch_size=batch_size), \n","                        num_workers=2, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T16:53:50.648110Z","iopub.status.busy":"2023-12-04T16:53:50.647860Z","iopub.status.idle":"2023-12-04T16:53:50.665149Z","shell.execute_reply":"2023-12-04T16:53:50.664273Z","shell.execute_reply.started":"2023-12-04T16:53:50.648088Z"},"trusted":true},"outputs":[],"source":["\n","class Encoder(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(Encoder, self).__init__()\n","        self.conv_layers = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 128, 4, 2, 1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 256, 4, 2, 1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(256, 512, 4, 2, 1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # Additional layers can be added here if needed\n","        )\n","        self.conv6_mu = nn.Conv2d(512, latent_dim, 4, 2, 1)\n","        self.conv6_logvar = nn.Conv2d(512, latent_dim, 4, 2, 1)\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)\n","        mu = self.conv6_mu(x)\n","        logvar = self.conv6_logvar(x)\n","        return mu, logvar\n","\n","class Decoder(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(Decoder, self).__init__()\n","        self.deconv_layers = nn.Sequential(\n","            nn.ConvTranspose2d(latent_dim, 512, 4, 2, 1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose2d(64, 3, 4, 2, 1),\n","            nn.Tanh()  # Use nn.Sigmoid() if your data is normalized to [0,1]\n","        )\n","\n","    def forward(self, z):\n","        z = self.deconv_layers(z)\n","        return z\n","\n","class VAE(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(VAE, self).__init__()\n","        self.encoder = Encoder(latent_dim)\n","        self.decoder = Decoder(latent_dim)\n","\n","    def reparameterize(self, mu, logvar):\n","        logvar = torch.clamp(logvar, -30, 20)\n","        variance = logvar.exp()\n","        stdev = variance.sqrt()\n","        eps = torch.randn_like(stdev)\n","        z = mu + eps * stdev\n","        return z\n","\n","    def forward(self, x):\n","        mu, logvar = self.encoder(x)\n","        z = self.reparameterize(mu, logvar)\n","        x_recon = self.decoder(z)\n","        return x_recon, mu, logvar"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T16:53:50.666646Z","iopub.status.busy":"2023-12-04T16:53:50.666269Z","iopub.status.idle":"2023-12-04T16:53:53.616089Z","shell.execute_reply":"2023-12-04T16:53:53.615084Z","shell.execute_reply.started":"2023-12-04T16:53:50.666615Z"},"trusted":true},"outputs":[],"source":["latent_dim = 128\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","vae = VAE(latent_dim).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T16:53:53.617751Z","iopub.status.busy":"2023-12-04T16:53:53.617367Z","iopub.status.idle":"2023-12-04T16:53:53.628511Z","shell.execute_reply":"2023-12-04T16:53:53.627537Z","shell.execute_reply.started":"2023-12-04T16:53:53.617715Z"},"trusted":true},"outputs":[],"source":["def add_random_white_box_and_get_mask_batch(images):\n","    \"\"\"\n","    Adds a random white box to each image in the batch and returns the images and masks.\n","    Images is a PyTorch tensor of shape (B, C, H, W).\n","    The mask is a binary tensor of the same batch size, height, and width as the images.\n","    \"\"\"\n","    B, C, H, W = images.shape\n","    masks = torch.zeros((B, H, W), dtype=torch.float32, device=images.device)\n","\n","    for i in range(B):\n","        if torch.rand(1).item() < 0.5:\n","            images[i], masks[i] = add_random_white_box_and_get_mask(images[i])\n","        else:\n","            masks[i] = torch.ones((B, H, W), dtype=torch.float32, device=images.device)\n","\n","    return images, masks\n","\n","def add_random_white_box_and_get_mask(image):\n","    C, H, W = image.shape\n","    box_width = int(torch.rand(1).item() * (0.5 * W) + 0.3 * W)\n","    box_height = int(torch.rand(1).item() * (0.5 * H) + 0.3 * H)\n","    x_start = int(torch.rand(1).item() * (W - box_width))\n","    y_start = int(torch.rand(1).item() * (H - box_height))\n","\n","    mask = torch.zeros((H, W), dtype=torch.float32, device=image.device)\n","    mask[y_start:y_start + box_height, x_start:x_start + box_width] = 1\n","    image[:, y_start:y_start + box_height, x_start:x_start + box_width] = 1.0\n","\n","    return image, mask"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T17:47:16.567987Z","iopub.status.busy":"2023-12-04T17:47:16.567167Z","iopub.status.idle":"2023-12-04T21:31:55.190358Z","shell.execute_reply":"2023-12-04T21:31:55.189369Z","shell.execute_reply.started":"2023-12-04T17:47:16.567952Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import io\n","from PIL import Image\n","import copy\n","\n","# Assuming 'vae' is your model and 'train_dataset' is your dataset\n","# Make sure to define 'vae', 'train_dataset', 'transforms', 'device', and 'batch_size' correctly\n","\n","optimizer = optim.Adam(vae.parameters(), lr=0.00002)\n","\n","# Initialize EMA model\n","ema_vae = copy.deepcopy(vae)\n","alpha = 0.99  # Smoothing factor for EMA\n","\n","def update_ema(ema_model, model, alpha):\n","    with torch.no_grad():\n","        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n","            ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n","\n","def loss_function(recon_x, x, mu, logvar):\n","    MSE = F.mse_loss(recon_x, x, reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return MSE + KLD\n","\n","# Training loop\n","num_epochs = 5\n","clip_value = 1.0  # Gradient clipping threshold\n","\n","for epoch in range(num_epochs):\n","    vae.train()\n","    train_loss = 0\n","    batch_idx = 0\n","    batch = []\n","\n","    for item in train_dataset:\n","        # Extract the image from the 'target' attribute\n","        image_bytes = io.BytesIO(item['target']['bytes'])\n","        image = Image.open(image_bytes).convert('RGB')\n","\n","        # Apply transformations\n","        image = transforms(image)  # Make sure this is a PIL image or convert it to one\n","\n","        # Accumulate batch\n","        batch.append(image.to(device))\n","\n","        if len(batch) == batch_size:\n","            # Process full batch\n","            batch_idx += 1\n","            images = torch.stack(batch)\n","            batch = []  # Reset for next batch\n","\n","            optimizer.zero_grad()\n","            recon_batch, mu, logvar = vae(images)\n","            loss = loss_function(recon_batch, images, mu, logvar)\n","\n","            loss.backward()\n","\n","            # Implementing Gradient Clipping\n","            torch.nn.utils.clip_grad_norm_(vae.parameters(), clip_value)\n","\n","            train_loss += loss.item()\n","            optimizer.step()\n","\n","            # Update EMA parameters\n","            update_ema(ema_vae, vae, alpha)\n","\n","            if batch_idx % 30 == 0:\n","                print('Train Epoch: {} [Batch {}] \\tLoss: {:.6f}'.format(\n","                    epoch, batch_idx, loss.item() / batch_size))\n","\n","    print('====> Epoch: {} Total loss: {:.4f}'.format(epoch, train_loss))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T21:31:55.193323Z","iopub.status.busy":"2023-12-04T21:31:55.192972Z","iopub.status.idle":"2023-12-04T21:31:55.248812Z","shell.execute_reply":"2023-12-04T21:31:55.247918Z","shell.execute_reply.started":"2023-12-04T21:31:55.193291Z"},"trusted":true},"outputs":[],"source":["torch.save({\n","    'vae': vae.state_dict(),\n","}, '/kaggle/working/weights_10.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T21:31:55.250725Z","iopub.status.busy":"2023-12-04T21:31:55.250338Z","iopub.status.idle":"2023-12-05T04:40:29.084827Z","shell.execute_reply":"2023-12-05T04:40:29.082534Z","shell.execute_reply.started":"2023-12-04T21:31:55.250690Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T04:40:34.069854Z","iopub.status.busy":"2023-12-05T04:40:34.069491Z","iopub.status.idle":"2023-12-05T04:40:34.126748Z","shell.execute_reply":"2023-12-05T04:40:34.125943Z","shell.execute_reply.started":"2023-12-05T04:40:34.069824Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-05T04:40:29.100382Z","iopub.status.idle":"2023-12-05T04:40:29.100748Z","shell.execute_reply":"2023-12-05T04:40:29.100575Z","shell.execute_reply.started":"2023-12-05T04:40:29.100559Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-05T04:40:29.102236Z","iopub.status.idle":"2023-12-05T04:40:29.102735Z","shell.execute_reply":"2023-12-05T04:40:29.102510Z","shell.execute_reply.started":"2023-12-05T04:40:29.102481Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-03T23:52:22.586340Z","iopub.status.busy":"2023-12-03T23:52:22.585499Z","iopub.status.idle":"2023-12-03T23:52:23.793541Z","shell.execute_reply":"2023-12-03T23:52:23.792278Z","shell.execute_reply.started":"2023-12-03T23:52:22.586294Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-05T04:40:51.683794Z","iopub.status.busy":"2023-12-05T04:40:51.683108Z","iopub.status.idle":"2023-12-05T04:40:52.994938Z","shell.execute_reply":"2023-12-05T04:40:52.993959Z","shell.execute_reply.started":"2023-12-05T04:40:51.683762Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from torchvision import transforms\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","\n","def test_image(image_path, model, device):\n","    model.eval()\n","    url = 'https://th.bing.com/th/id/OIP.L4nUSvQ7ZaefejVVEkLG5QHaEp?rs=1&pid=ImgDetMain'\n","    # Fetch the image from the URL\n","    response = requests.get(url)\n","    \n","    imgTransform = torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Resize(512),\n","        #torchvision.transforms.RandomCrop(512),\n","    ])\n","\n","    # Ensure the request was successful\n","    if response.status_code == 200:\n","        # Open the image from the bytes in memory\n","        image = Image.open(BytesIO(response.content))\n","    else:\n","        print(\"Failed to retrieve the image\")\n","    # Apply transformations and add batch dimension\n","    image_tensor = imgTransform(image).unsqueeze(0).to(device)\n","\n","    # Generate the fake image\n","    with torch.no_grad():\n","        recon_batch, mu, logvar = vae(image_tensor)\n","\n","    # Convert tensors back to images for visualization\n","    recon_image = transforms.ToPILImage()(recon_batch.squeeze().cpu())  # Denormalize if the model uses [-1, 1] range\n","    real_image = transforms.ToPILImage()(image_tensor.squeeze().cpu())     # Denormalize if the model uses [-1, 1] range\n","\n","    # Plotting\n","    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n","\n","    ax[0].imshow(real_image)\n","    ax[0].set_title(\"Input Image\")\n","    ax[0].axis(\"off\")\n","\n","    ax[1].imshow(recon_image)\n","    ax[1].set_title(\"Generated Image\")\n","    ax[1].axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","    \n","test_image(\"\", vae, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T16:53:53.630510Z","iopub.status.busy":"2023-12-04T16:53:53.629741Z","iopub.status.idle":"2023-12-04T16:53:53.682736Z","shell.execute_reply":"2023-12-04T16:53:53.681802Z","shell.execute_reply.started":"2023-12-04T16:53:53.630477Z"},"trusted":true},"outputs":[],"source":["checkpoint = torch.load(\"/vae.pth\")\n","vae.load_state_dict(checkpoint['vae'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30588,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
